{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "65078c11-a885-4b4b-959b-9d91be296c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "8408ea20-2646-48b5-8e96-18f72c858b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\johnny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "from io import BytesIO\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import chromadb\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Ensure that the Punkt Tokenizer Models are downloaded\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "25be8dcc-9708-4998-875e-32e549502c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\downloads\\amazon_customer_reviews\n"
     ]
    }
   ],
   "source": [
    "#download_dir = 'D:\\\\downloads'\n",
    "download_dir = 'D:\\\\downloads\\\\amazon_customer_reviews'\n",
    "print(download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "8a93576f-77f7-4ea8-9b95-a23c33029c3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\downloads\\\\amazon_customer_reviews\\\\amazon_reviews_pickle_paragraphs_20240219.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[407], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#df_pickle_paragraphs_filename = os.path.join(download_dir,\"amazon_reviews_pickle_paragraphs_20240129.pkl\")\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df_pickle_paragraphs_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_dir,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamazon_reviews_pickle_paragraphs_20240219.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m df_paragraph \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_pickle_paragraphs_filename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py:189\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    188\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\downloads\\\\amazon_customer_reviews\\\\amazon_reviews_pickle_paragraphs_20240219.pkl'"
     ]
    }
   ],
   "source": [
    "#df_pickle_paragraphs_filename = os.path.join(download_dir,\"amazon_reviews_pickle_paragraphs_20240129.pkl\")\n",
    "df_pickle_paragraphs_filename = os.path.join(download_dir,\"amazon_reviews_pickle_paragraphs_20240219.pkl\")\n",
    "df_paragraph = pd.read_pickle(df_pickle_paragraphs_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b7b602-72dd-48e2-9f0f-71ab994a03cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraph.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f3346-5c5d-4157-ae0a-5791668ab9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraph.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b3358f-8781-4f89-8299-92d28eae3d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraph.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a57ed4-e31d-427e-a522-e527ecf88ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pickle_sentences_filename = os.path.join(download_dir,\"amazon_reviews_pickle_sentences_20240129.pkl\")\n",
    "df_pickle_sentences_filename = os.path.join(download_dir,\"amazon_reviews_pickle_sentences_20240219.pkl\")\n",
    "\n",
    "df_sentences = pd.read_pickle(df_pickle_sentences_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40667328-5673-43ac-869c-cf8ebc4d3d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14820fbf-3f6e-4406-9309-11c26dcec6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e01a2-886d-4b6e-b28f-fd2e6de147c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571836bd-810b-4713-a097-f226e6cb1726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.iloc[5]['Summary_vector'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513ec3c-cfab-4ed9-8b12-e2ffb59f90db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.iloc[5]['Summary_vector'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2aef16-ad91-4c3c-9b60-b321242e6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_sentences.iloc[5]['Summary_vector'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34126ab-8d2e-40c4-be0d-9d3766c6a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9468d5f-30b3-435b-9e13-70156311bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_sentences.iloc[0:2].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89bc1b4-ba30-4406-8c72-de711ac1a285",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "# df = pd.DataFrame({'Summary_vector': [... your data ...]})\n",
    "\n",
    "# Process the 'Summary_vector' column\n",
    "def process_vector(v):\n",
    "    # Convert to numpy array with dtype float32\n",
    "    v_array = np.array(v, dtype=np.float32)\n",
    "    \n",
    "    # Optionally reshape to 2D (1, -1) if required by your application\n",
    "    # v_array = v_array.reshape(1, -1)\n",
    "    \n",
    "    # Return processed vector\n",
    "    return v_array\n",
    "\n",
    "# Apply processing to each vector in the column\n",
    "df['Processed_vector'] = df['Summary_vector'].apply(process_vector)\n",
    "\n",
    "# Check the result for the first few rows\n",
    "print(df['Processed_vector'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec911ebc-e0f0-47d6-be7a-7fe9eeca0380",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_vector(v):\n",
    "    return np.array(v, dtype=np.float32).flatten()\n",
    "\n",
    "# Apply preprocessing to each vector column\n",
    "df_sentences['Summary_vector'] = df_sentences['Summary_vector'].apply(preprocess_vector)\n",
    "df_sentences['Sentence_vector'] = df_sentences['Sentence_vector'].apply(preprocess_vector)\n",
    "df_paragraph['Paragraph_vector'] = df_paragraph['Paragraph_vector'].apply(preprocess_vector)\n",
    "# Repeat for the third vector column if exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d21b1-472b-4242-931c-5243330c6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "def create_faiss_index_old(vectors):\n",
    "    dimension = vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(vectors)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71decd9-f675-40c5-aaaa-960592d84fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index_old(vectors):\n",
    "    # Example setup for FAISS index - this is conceptual\n",
    "    index = faiss.IndexFlatL2(vectors.shape[1])\n",
    "    # Example of batch addition with tqdm progress bar\n",
    "    for i in tqdm(range(0, vectors.shape[0], 1000), desc=\"Adding vectors to FAISS index\"):\n",
    "        index.add(vectors[i:i+1000])\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1af0988-1da8-453f-9f8e-4bbdded4b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sentences.iloc[0]['Summary_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ae05f3-872b-425c-ad96-f2d3af0b56ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sentences.iloc[167494]['Summary_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030818cc-e921-4806-8d51-b60a6e56df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraph.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf7443-1d29-42c5-9b96-e0175149861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.iloc[167494]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb0ff2-7bf7-4b47-b529-9520f2b2c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraph = df_paragraph.dropna(subset=['Paragraph', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659ba89-e75d-4c97-afac-4abfc508e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original df_sentences rows: {len(df_sentences)}\")\n",
    "# Drop rows where 'Summary' is NaN\n",
    "df_sentences= df_sentences.dropna(subset=['Summary'])\n",
    "\n",
    "# Optionally, also drop rows where 'Sentence' is NaN if relevant to your analysis\n",
    "df_sentences = df_sentences.dropna(subset=['Sentence'])\n",
    "\n",
    "print(f\"Cleaned df_sentences rows: {len(df_sentences)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1ecf4-24a8-466c-a82c-f7f79521c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_faiss_index(vectors, n_clusters):\n",
    "    dimension = vectors.shape[1]  # Dimension of the vectors\n",
    "    kmeans = faiss.Kmeans(dimension, n_clusters, niter=20, verbose=True, spherical=False)\n",
    "    kmeans.train(vectors)\n",
    "    return kmeans\n",
    "\n",
    "def cluster_vectors_and_save(df, vector_column, n_clusters, cluster_col_name, distance_col_name):\n",
    "    # Filter out NaNs and prepare the vectors\n",
    "    filtered_df = df.dropna(subset=[vector_column])\n",
    "    vectors = np.vstack(filtered_df[vector_column].apply(np.array).to_list())\n",
    "\n",
    "    # Initialize the progress bar\n",
    "    tqdm.pandas(desc=f\"Clustering {vector_column}\")\n",
    "    \n",
    "    # Create a FAISS KMeans index for clustering\n",
    "    print(f\"Training KMeans for {vector_column}...\")\n",
    "    kmeans = create_faiss_index(vectors, n_clusters)\n",
    "\n",
    "    # Assign vectors to the nearest cluster and capture distances\n",
    "    print(f\"Assigning clusters for {vector_column}...\")\n",
    "    distances, cluster_assignments = kmeans.index.search(vectors, 1)\n",
    "\n",
    "    # Since we filtered NaNs, align results with the original DataFrame indices\n",
    "    valid_indices = filtered_df.index\n",
    "\n",
    "    # Update the original DataFrame\n",
    "    df.loc[valid_indices, cluster_col_name] = cluster_assignments.flatten()\n",
    "    df.loc[valid_indices, distance_col_name] = distances.flatten()\n",
    "\n",
    "    return df, kmeans\n",
    "\n",
    "# Example usage with a specified number of clusters\n",
    "n_clusters = 100\n",
    "\n",
    "# Assuming df_sentences and df_paragraph are your DataFrames and they are properly initialized\n",
    "df_sentences, index_summary  = cluster_vectors_and_save(df_sentences, 'Summary_vector', n_clusters, 'Summary_Cluster', 'Summary_Distance')\n",
    "df_sentences, index_sentence  = cluster_vectors_and_save(df_sentences, 'Sentence_vector', n_clusters, 'Sentence_Cluster', 'Sentence_Distance')\n",
    "df_paragraph, index_paragraph  = cluster_vectors_and_save(df_paragraph, 'Paragraph_vector', n_clusters, 'Paragraph_Cluster', 'Paragraph_Distance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a050a6c-86dd-429a-8dc1-68124215fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c63a280-1d3b-480b-8489-f0bdd2f7055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a7e91c-d4ea-4e87-b4ea-aabeab1c8721",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraph.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5748d0e-e48b-4b57-bb59-3a0c3bb34af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraph.iloc[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5cc813-00ca-4b96-b852-fb2448dd025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 12\n",
    "cluster_0_df = df_paragraph[df_paragraph['Paragraph_Cluster'] == cluster]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e52e7-7055-4d4f-a20f-66e42e5ef2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7addd8-36d4-425f-870b-90f636e1bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows in each cluster for paragraph vectors\n",
    "paragraph_cluster_counts = df_paragraph['Paragraph_Cluster'].value_counts().sort_index()\n",
    "#print(\"Paragraph Cluster Distribution:\\n\", paragraph_cluster_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83145315-3f96-4dec-908b-b132cfcd14f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows in each cluster for summary vectors\n",
    "summary_cluster_counts = df_sentences['Summary_Cluster'].value_counts().sort_index()\n",
    "#print(\"Summary Cluster Distribution:\\n\", summary_cluster_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a04666-1962-4275-9b80-612582ddd823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows in each cluster for sentence vectors\n",
    "sentence_cluster_counts = df_sentences['Sentence_Cluster'].value_counts().sort_index()\n",
    "#print(\"Sentence Cluster Distribution:\\n\", sentence_cluster_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a6cff-1225-4a91-a046-af8aa26a31d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the distribution of paragraph clusters\n",
    "paragraph_cluster_counts.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Distribution of Paragraph Clusters')\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('Number of Rows')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3bf423-9517-4743-b316-9407d564dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, print some sample paragraphs to get a feel for the content\n",
    "for paragraph in cluster_0_df['Paragraph'].sample(n=5).values:\n",
    "    print(paragraph, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b184359-2430-44e7-aa0a-6d6c80c892e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_stats = cluster_0_df['Paragraph_Distance'].describe()\n",
    "print(distance_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa7cfc-3dcc-4d72-81a8-15a921d62f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192035cb-5282-43f2-895f-9fad57c3da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Concatenate all paragraphs in the cluster into a single string\n",
    "text = \" \".join(paragraph for paragraph in cluster_0_df['Paragraph'])\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea9ef8-5ead-4548-9c5e-74b65595df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5671c4-d32f-4e86-b2c1-a8dbba18f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(wordcloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d0c783-6e12-400d-ac65-61f51af4c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word frequencies:\", wordcloud.words_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931fa628-8660-49ff-bbe3-947d24d76d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Word layout:\", wordcloud.layout_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06d3a7-085c-444c-ab0f-00bfc28ecf83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Latent Dirichlet Allocation (LDA)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee362f5-6d07-4095-baa1-7049bdc84cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0_df = df_paragraph[df_paragraph['Paragraph_Cluster'] == cluster]\n",
    "texts = cluster_0_df['Paragraph'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875886c1-a7b6-4bc4-a0ac-edff6b592fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Ensure you have the necessary NLTK data\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tokenization and stop words removal\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "texts_tokenized = [[word for word in tokenizer.tokenize(text.lower()) if word not in stop_words] for text in texts]\n",
    "\n",
    "# Creating a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(texts_tokenized)\n",
    "\n",
    "# Filtering out extremes to limit the number of features\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "# Creating the document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_tokenized]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81202abf-c606-4c35-9a19-964b4a7a4e68",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10370dff-ae42-457a-8db2-0f4761453b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 50  # Adjust based on your dataset and needs\n",
    "ldamodel = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "\n",
    "# Print the topics\n",
    "#topics = ldamodel.print_topics(num_words=4)\n",
    "#for topic in topics:\n",
    "#    print(topic)\n",
    "# Print all 50 topics with the top 4 words in each\n",
    "all_topics = ldamodel.print_topics(num_topics=50, num_words=4)\n",
    "for topic in all_topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb3f66-17b7-4a95-a52c-d72b886461e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc66d4f-307f-4185-aa4f-0f985f42a9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948bff2-d3ae-4769-9098-6b58813571db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0b714b-016e-44cc-aae9-7eb4567301ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b714b-29ae-4240-b32f-f81b314eeb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# way slow\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def find_top_similarities_and_distances(index, vectors, k=21):\n",
    "    \"\"\"\n",
    "    Find the top k-1 most similar items and their distances for each vector in the given index.\n",
    "    Exclude the first result to skip the query item itself.\n",
    "    \"\"\"\n",
    "    similar_indices = []\n",
    "    distances_list = []\n",
    "    \n",
    "    # Perform the search for each vector\n",
    "    for i in tqdm(range(vectors.shape[0]), desc='Finding Similarities and Distances'):\n",
    "        D, I = index.search(vectors[i:i+1], k)  # Search for the top k similar items\n",
    "        similar_indices.append(I[0][1:])  # Skip the first result and keep the next 20\n",
    "        distances_list.append(D[0][1:])  # Skip the distance to itself and keep the next 20\n",
    "    \n",
    "    return similar_indices, distances_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb137bdc-31d3-4f94-b99f-46485636eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# way slow 128 days each\n",
    "# Example for the summary vectors and index\n",
    "top20_summary_indices, top20_summary_distances = find_top_similarities_and_distances(index_summary, summary_vectors, k=21)\n",
    "\n",
    "# Update the DataFrame\n",
    "df_sentences['Top20_Summary_Indices'] = top20_summary_indices\n",
    "df_sentences['Top20_Summary_Distances'] = top20_summary_distances\n",
    "\n",
    "# Repeat for sentence and paragraph vectors and indexes\n",
    "top20_sentence_indices, top20_sentence_distances = find_top_similarities_and_distances(index_sentence, sentence_vectors, k=21)\n",
    "df_sentences['Top20_Sentence_Indices'] = top20_sentence_indices\n",
    "df_sentences['Top20_Sentence_Distances'] = top20_sentence_distances\n",
    "\n",
    "top20_paragraph_indices, top20_paragraph_distances = find_top_similarities_and_distances(index_paragraph, paragraph_vectors, k=21)\n",
    "df_paragraph['Top20_Paragraph_Indices'] = top20_paragraph_indices\n",
    "df_paragraph['Top20_Paragraph_Distances'] = top20_paragraph_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1849ce-e312-44be-92a7-560130a44363",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop # some of the code below duplicates the code in the above block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a90212-4a20-4318-8aff-54ec2f32e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pickle_sentences_filename_clusters = os.path.join(download_dir,\"amazon_reviews_pickle_sentences_20240129_clusters.pkl\")\n",
    "\n",
    "df_sentences.to_pickle(df_pickle_sentences_filename_clusters)  # Python pickle format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711865a6-0660-436c-9a6c-297ee9eb3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pickle_paragraph_filename_clusters = os.path.join(download_dir,\"amazon_reviews_pickle_paragraph_20240129_clusters.pkl\")\n",
    "\n",
    "df_paragraph.to_pickle(df_pickle_paragraph_filename_clusters)  # Python pickle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2b5ef5-d5a8-45ad-80f6-9c66ec22d99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04e6211-da9a-4eb9-8673-d4c7af9cfd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_sentences.iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89041738-bdc7-457c-a85a-995ea84c3fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f744f3b-4f9e-4b30-843b-31e0ff91ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]['Summary_vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21388b9-0491-4810-ab5a-4d4aa53bb063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Retrieve vectors from your DataFrame\n",
    "vector1 = df.iloc[0]['Processed_vector']\n",
    "vector2 = df.iloc[1]['Processed_vector']\n",
    "\n",
    "# Ensure both vectors are numpy arrays and have the same shape\n",
    "vector1 = np.array(vector1, dtype=np.float32).reshape(1, -1)\n",
    "vector2 = np.array(vector2, dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "# Verify shapes are correct (this step is just for demonstration and can be removed later)\n",
    "print(f\"Vector1 Shape: {vector1.shape}\")\n",
    "print(f\"Vector2 Shape: {vector2.shape}\")\n",
    "\n",
    "# Combine vectors into a single 2D array\n",
    "vectors = np.vstack((vector1, vector2))\n",
    "\n",
    "# Continue with FAISS index creation and querying\n",
    "dimension = vectors.shape[1]  # Dimension of the vectors\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance for similarity measure\n",
    "index.add(vectors)  # Add vectors to the index\n",
    "\n",
    "# Perform a query with the first vector\n",
    "query_vector = np.array([vector1], dtype=np.float32).reshape(1, -1)\n",
    "k = 2  # Find the 2 nearest neighbors, including the query vector itself\n",
    "distances, indices = index.search(query_vector, k)\n",
    "\n",
    "# Display the results\n",
    "print(\"Distances:\", distances)\n",
    "print(\"Indices:\", indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55350ae4-33e2-44f6-b691-2862d62d1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Prepare the vectors\n",
    "# Assuming vector1 is the one you provided and vector2 is similarly obtained\n",
    "vector1 = [1.07387170e-01,  1.54322982e-02, ...]  # Truncated for brevity\n",
    "vector2 = [ ... ]  # Similar structure to vector1\n",
    "vector1 = df.iloc[0]['Summary_vector']\n",
    "vector2 = df.iloc[1]['Summary_vector']\n",
    "\n",
    "vectors = np.array([vector1, vector2], dtype=np.float32)\n",
    "\n",
    "# Step 2: Create a FAISS index\n",
    "dimension = vectors.shape[1]  # Dimension of the vectors\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance for similarity measure\n",
    "\n",
    "# Step 3: Add the vectors to the index\n",
    "index.add(vectors)\n",
    "\n",
    "# Step 4: Perform a query\n",
    "# Use the first vector as a query to find its nearest neighbor\n",
    "# Note: Including itself, so the nearest neighbor will be itself for k=1\n",
    "query_vector = np.array([vector1], dtype=np.float32)\n",
    "k = 2  # Find the 2 nearest neighbors, including the query vector itself\n",
    "distances, indices = index.search(query_vector, k)\n",
    "\n",
    "# Display the results\n",
    "print(\"Distances:\", distances)\n",
    "print(\"Indices:\", indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e921ae5-3d4e-4249-91e0-2dfc2b1f4f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033425f-cdfb-4f58-8d41-af57e1010d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiment = df_sentences.head(1000).copy()  # Assuming 'df' is your original DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3b425-a454-432c-848f-50a853b86d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_vectors_for_faiss(vector):\n",
    "    # Ensure the vector is a numpy array of type float32 and is 2D\n",
    "    if isinstance(vector, list):  # If the vector is stored as a list\n",
    "        vector = np.array(vector, dtype=np.float32)\n",
    "    if vector.ndim == 1:\n",
    "        vector = vector.reshape(1, -1)\n",
    "    return vector\n",
    "\n",
    "# Apply this function to the 'Summary_vector' column\n",
    "df_experiment['Summary_vector'] = df_experiment['Summary_vector'].apply(preprocess_vectors_for_faiss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76c4864-2f59-4a35-beb2-30473d5f39ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Assuming you have preprocessed the vectors as 2D arrays\n",
    "# Flatten the arrays for FAISS index, as it expects flat arrays\n",
    "vectors = np.vstack(df_experiment['Summary_vector'].to_numpy())\n",
    "\n",
    "# Create a FAISS index\n",
    "dimension = vectors.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(vectors.astype(np.float32))  # Ensure data is float32\n",
    "\n",
    "# Query the index (example)\n",
    "query_vector = vectors[0:1]  # Use the first vector as a query example\n",
    "k = 5  # Number of nearest neighbors to find\n",
    "distances, indices = index.search(query_vector, k)\n",
    "\n",
    "print(distances, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541afe1-878e-4402-bd21-ba91fcd41377",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf6c6d-dfb2-4727-98fa-ce35b1cf2435",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea5dd36-07f8-45c3-9d93-1674e9ba09e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vectors = index.ntotal\n",
    "vector_dim = index.d\n",
    "print(f\"Number of vectors: {n_vectors}, Dimensionality: {vector_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a062da5c-5510-4049-8471-9397f7e4f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_trained = index.is_trained\n",
    "print(f\"Is the index trained? {is_trained}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355be544-b3c2-4204-8f93-b1aed537f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(index, faiss.IndexIVFFlat):\n",
    "    print(f\"Number of list: {index.nlist}\")\n",
    "    print(f\"Number of probes: {index.nprobe}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e6602-3f72-4416-b36c-4095901ff3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the index to disk\n",
    "index_file = \"my_index.faiss\"\n",
    "faiss.write_index(index, index_file)\n",
    "\n",
    "# Load the index (for inspection or use in another program)\n",
    "index_loaded = faiss.read_index(index_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6ef2e-032d-4a6a-a76b-025b771c7c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MinIO client and BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def generate_vectors(text):\n",
    "    # Check if GPU is available and use it; otherwise, use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Send model to device (GPU or CPU)\n",
    "    model.to(device)\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "    # Ensure no gradient calculations\n",
    "    with torch.no_grad():\n",
    "        # Prepare inputs and send them to the device\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        # Forward pass, send model outputs back to CPU\n",
    "        outputs = model(**inputs).last_hidden_state.mean(dim=1).squeeze().to('cpu').numpy()\n",
    "\n",
    "    # Convert the output to float32 for compatibility and ensure it's flat\n",
    "    return outputs.astype(np.float32)\n",
    "\n",
    "# Example usage with a DataFrame\n",
    "# df_sentence['Summary_vector'] = df_sentence['Summary'].progress_apply(lambda x: generate_vectors(x) if isinstance(x, str) else np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e449fd-34b3-44b9-8ef4-1fbaa6a788f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbbe912-f33b-4617-97cd-4c27c3d50893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3942d041-205a-4c8b-94b8-0afd21e86eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.stack(df_sentences['Summary_vector'].values).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97864dba-287b-419d-be47-d54a8cc0c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = df_sentences['Summary_vector'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2777360-c6ad-4ddf-896b-5786e5dad98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16edf25e-24b4-44c7-b601-2b278dd0b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = df_sentences['Summary_vector'].values.astype(np.float32)  # Assuming vectors are stored as numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1902d14a-dd14-4685-bdfb-4d6fc4b56a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ff635-1acd-4252-9496-5ce53e20392d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973b2975-027b-43da-a400-bed66fbdc7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582bb508-2fd5-40c7-adaa-94ba012c7492",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838fad9-aa7a-49d2-b6de-f961ab17791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Initialize the Chroma client with a specified database path\n",
    "db_path = download_dir  # Specify the path where you want to store the ChromaDB\n",
    "#chroma_client = chromadb.Client(path=db_path) # this gave an error\n",
    "#client = chromadb.PersistentClient(path=\"/path/to/save/to\") # from web site\n",
    "chroma_client = chromadb.PersistentClient(path=db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb899ea-ee4c-48a0-8d73-565338ef377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_metadata_and_vectors(df):\n",
    "    vector_columns = [col for col in df.columns if 'vector' in col.lower()]\n",
    "    metadata_columns = [col for col in df.columns if 'vector' not in col.lower()]\n",
    "    return metadata_columns, vector_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65784f74-d1ff-4147-99cb-54667e2e3216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b38e7a7-bb5d-46bc-8410-2dc87080af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_to_chromadb(df, collection_name, text_columns, embedding_columns, db_path, ignore_columns=None):\n",
    "    if ignore_columns is None:\n",
    "        ignore_columns = []\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=db_path)\n",
    "\n",
    "    try:\n",
    "        collection = chroma_client.get_collection(collection_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Collection {collection_name} not found, creating a new one. Error: {e}\")\n",
    "        collection = chroma_client.create_collection(name=collection_name)\n",
    "\n",
    "    #for _, row in df.iterrows():\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Loading to {collection_name}\"):\n",
    "        metadata = {col: row[col] for col in row.index if col not in embedding_columns + text_columns + ignore_columns}\n",
    "        #embeddings = [row[col] for col in embedding_columns if col in row and col not in ignore_columns]\n",
    "\n",
    "        # Ensure embeddings are converted to list if they are numpy arrays\n",
    "        embeddings = [row[col].tolist() for col in embedding_columns if col in row and col not in ignore_columns and isinstance(row[col], np.ndarray)][0]\n",
    "        embeddings = embeddings[0] if embeddings else None\n",
    "        if embeddings is not None:\n",
    "            embeddings_list.append(embeddings)\n",
    "        \n",
    "        documents = [row[col] for col in text_columns if col in row]\n",
    "        \n",
    "        document_id = f\"{collection_name}_{row['Id']}\"\n",
    "        if 'P_index' in row and 'S_sentence_number' in row:\n",
    "            document_id += f\"_{row['P_index']}_{row['S_sentence_number']}\"\n",
    "\n",
    "        collection.add(\n",
    "            documents=documents,\n",
    "            embeddings=embeddings,  # Each embedding is added separately\n",
    "            metadatas=[metadata],\n",
    "            ids=[document_id]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754b20d-dfe7-4b77-88a4-327d09fa297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_collection(collection):\n",
    "    try:\n",
    "        chroma_client.delete_collection(collection)\n",
    "        print(\"Collection '{collection}' deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete collection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0f884-938d-4e1c-bb88-e0cd47bd68ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_collection('sentences_collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aa9710-cb6e-4c0c-9e35-8d977151fc14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e30de76-d0e9-42fc-886b-2c6090d45b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collections\n",
    "\n",
    "delete_collection('sentences_collection')\n",
    "sentences_collection = chroma_client.create_collection(name=\"sentences_collection\")\n",
    "\n",
    "\n",
    "delete_collection('paragraphs_collection')\n",
    "paragraphs_collection = chroma_client.create_collection(name=\"paragraphs_collection\")\n",
    "\n",
    "\n",
    "delete_collection('paragraphs_collection')\n",
    "summary_collection = chroma_client.create_collection(name=\"paragraphs_collection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9c647-b793-4f2f-bc66-12f88c382407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(df_sentences.iloc[0]['Sentence_vector'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4950d653-f3e9-4b72-81e9-2d1a7cd116e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sentences.iloc[0]['Sentence_vector'][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f4d98-c455-4963-89fc-15cd42ff3349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
