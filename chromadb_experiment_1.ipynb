{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "65078c11-a885-4b4b-959b-9d91be296c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72122cf4-be0c-48dc-85d9-b4dc425499b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "8408ea20-2646-48b5-8e96-18f72c858b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\johnny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "from io import BytesIO\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import chromadb\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Ensure that the Punkt Tokenizer Models are downloaded\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "25be8dcc-9708-4998-875e-32e549502c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\downloads\\amazon_customer_reviews\n"
     ]
    }
   ],
   "source": [
    "#download_dir = 'D:\\\\downloads'\n",
    "download_dir = 'D:\\\\downloads\\\\amazon_customer_reviews'\n",
    "print(download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "8a93576f-77f7-4ea8-9b95-a23c33029c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pickle_paragraphs_filename = os.path.join(download_dir,\"amazon_reviews_pickle_paragraphs_20240129.pkl\")\n",
    "df_pickle_paragraphs_filename = os.path.join(download_dir,\"amazon_reviews_pickle_paragraphs_20240219.pkl\")\n",
    "df_paragraph = pd.read_pickle(df_pickle_paragraphs_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "b0b7b602-72dd-48e2-9f0f-71ab994a03cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Paragraph',\n",
       "       'Paragraph_vector', 'Summary_vector'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paragraph.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "cd6f3346-5c5d-4157-ae0a-5791668ab9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568427, 12)"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paragraph.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "e6b3358f-8781-4f89-8299-92d28eae3d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                                                                        6\n",
       "ProductId                                                        B006K2ZZ7K\n",
       "UserId                                                        ADT0SRK1MGOEU\n",
       "ProfileName                                                  Twoapennything\n",
       "HelpfulnessNumerator                                                      0\n",
       "HelpfulnessDenominator                                                    0\n",
       "Score                                                                     4\n",
       "Time                                                             1342051200\n",
       "Summary                                                          Nice Taffy\n",
       "Paragraph                 I got a wild hair for taffy and ordered this f...\n",
       "Paragraph_vector          [-0.08328723, 0.08941264, 0.39326283, 0.106011...\n",
       "Summary_vector            [0.45653412, -0.095085524, 0.59296936, -0.1120...\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paragraph.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "84a57ed4-e31d-427e-a522-e527ecf88ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pickle_sentences_filename = os.path.join(download_dir,\"amazon_reviews_pickle_sentences_20240129.pkl\")\n",
    "df_pickle_sentences_filename = os.path.join(download_dir,\"amazon_reviews_pickle_sentences_20240219.pkl\")\n",
    "\n",
    "df_sentences = pd.read_pickle(df_pickle_sentences_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "40667328-5673-43ac-869c-cf8ebc4d3d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'P_index',\n",
       "       'S_sentence_number', 'Sentence', 'Summary_vector', 'Sentence_vector'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "14820fbf-3f6e-4406-9309-11c26dcec6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2832752, 14)"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "b94e01a2-886d-4b6e-b28f-fd2e6de147c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                                                                        3\n",
       "ProductId                                                        B000LQOCH0\n",
       "UserId                                                        ABXLMWJIXXAIN\n",
       "ProfileName                                 Natalia Corres \"Natalia Corres\"\n",
       "HelpfulnessNumerator                                                      1\n",
       "HelpfulnessDenominator                                                    1\n",
       "Score                                                                     4\n",
       "Time                                                             1219017600\n",
       "Summary                                               \"Delight\" says it all\n",
       "P_index                                                                 P_2\n",
       "S_sentence_number                                                       S_1\n",
       "Sentence                  This is a confection that has been around a fe...\n",
       "Summary_vector            [0.078089125, 0.12389954, 0.563005, 0.06509149...\n",
       "Sentence_vector           [0.19892824, -0.11812688, 0.24637115, 0.041541...\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "571836bd-810b-4713-a097-f226e6cb1726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences.iloc[5]['Summary_vector'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "8513ec3c-cfab-4ed9-8b12-e2ffb59f90db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences.iloc[5]['Summary_vector'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "8c2aef16-ad91-4c3c-9b60-b321242e6f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_sentences.iloc[5]['Summary_vector'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "d34126ab-8d2e-40c4-be0d-9d3766c6a22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'P_index',\n",
       "       'S_sentence_number', 'Sentence', 'Summary_vector', 'Sentence_vector'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "b9468d5f-30b3-435b-9e13-70156311bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_sentences.iloc[0:2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "ad2d21b1-472b-4242-931c-5243330c6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "def create_faiss_index_old(vectors):\n",
    "    dimension = vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(vectors)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "e71decd9-f675-40c5-aaaa-960592d84fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index_old(vectors):\n",
    "    # Example setup for FAISS index - this is conceptual\n",
    "    index = faiss.IndexFlatL2(vectors.shape[1])\n",
    "    # Example of batch addition with tqdm progress bar\n",
    "    for i in tqdm(range(0, vectors.shape[0], 1000), desc=\"Adding vectors to FAISS index\"):\n",
    "        index.add(vectors[i:i+1000])\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "f1af0988-1da8-453f-9f8e-4bbdded4b0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sentences.iloc[0]['Summary_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "60ae05f3-872b-425c-ad96-f2d3af0b56ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sentences.iloc[167494]['Summary_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "030818cc-e921-4806-8d51-b60a6e56df6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Paragraph',\n",
       "       'Paragraph_vector', 'Summary_vector'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paragraph.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "3ebf7443-1d29-42c5-9b96-e0175149861d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                                                                    33960\n",
       "ProductId                                                        B00412W76S\n",
       "UserId                                                       A20X23WJLV4R5X\n",
       "ProfileName                                                kittykatbookworm\n",
       "HelpfulnessNumerator                                                     18\n",
       "HelpfulnessDenominator                                                   18\n",
       "Score                                                                     5\n",
       "Time                                                             1251849600\n",
       "Summary                                              tasty Maybe beneficial\n",
       "P_index                                                             P_33959\n",
       "S_sentence_number                                                       S_1\n",
       "Sentence                  My midwife recommended this product, and I dra...\n",
       "Summary_vector            [0.034660093, -0.20093988, -0.12915072, 0.1708...\n",
       "Sentence_vector           [0.10867884, -0.22328994, 0.03946755, -0.17980...\n",
       "Name: 167494, dtype: object"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences.iloc[167494]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "0fbb0ff2-7bf7-4b47-b529-9520f2b2c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this deletes rows where paragraph or summary are nan (empty)\n",
    "df_paragraph = df_paragraph.dropna(subset=['Paragraph', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "2659ba89-e75d-4c97-afac-4abfc508e44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df_sentences rows: 2832752\n",
      "Cleaned df_sentences rows: 2832752\n"
     ]
    }
   ],
   "source": [
    "# this deletes rows where the summary or sentence is a nan (empty)\n",
    "print(f\"Original df_sentences rows: {len(df_sentences)}\") # how many rows are there to start\n",
    "# Drop rows where 'Summary' is NaN\n",
    "df_sentences= df_sentences.dropna(subset=['Summary'])\n",
    "\n",
    "# Optionally, also drop rows where 'Sentence' is NaN if relevant to your analysis\n",
    "df_sentences = df_sentences.dropna(subset=['Sentence'])\n",
    "\n",
    "print(f\"Cleaned df_sentences rows: {len(df_sentences)}\") # how many rows are there now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "a4d1ecf4-24a8-466c-a82c-f7f79521c8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KMeans for Sentence_vector...\n",
      "Assigning clusters for Sentence_vector...\n",
      "Training KMeans for Paragraph_vector...\n",
      "Assigning clusters for Paragraph_vector...\n",
      "Training KMeans for Summary_vector...\n",
      "Assigning clusters for Summary_vector...\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_faiss_index(vectors, n_clusters):\n",
    "    dimension = vectors.shape[1]  # Dimension of the vectors\n",
    "    kmeans = faiss.Kmeans(dimension, n_clusters, niter=20, verbose=True, spherical=False)\n",
    "    kmeans.train(vectors)\n",
    "    return kmeans\n",
    "\n",
    "def cluster_vectors_and_save(df, vector_column, n_clusters, cluster_col_name, distance_col_name):\n",
    "    # this takes in a dataframe, the number of clusters that you want to make, and 2 column names that you want to add to the dataframe\n",
    "    # the columns are to hold the cluster that the row is in\n",
    "    # Filter out NaNs and prepare the vectors\n",
    "    filtered_df = df.dropna(subset=[vector_column])\n",
    "    vectors = np.vstack(filtered_df[vector_column].apply(np.array).to_list())\n",
    "\n",
    "    # Initialize the progress bar\n",
    "    tqdm.pandas(desc=f\"Clustering {vector_column}\")\n",
    "    \n",
    "    # Create a FAISS KMeans index for clustering\n",
    "    print(f\"Training KMeans for {vector_column}...\")\n",
    "    kmeans = create_faiss_index(vectors, n_clusters)\n",
    "\n",
    "    # Assign vectors to the nearest cluster and capture distances\n",
    "    print(f\"Assigning clusters for {vector_column}...\")\n",
    "    distances, cluster_assignments = kmeans.index.search(vectors, 1)\n",
    "\n",
    "    # Since we filtered NaNs, align results with the original DataFrame indices\n",
    "    valid_indices = filtered_df.index\n",
    "\n",
    "    # Update the original DataFrame. This makes 2 new columns in the original dataframe\n",
    "    df.loc[valid_indices, cluster_col_name] = cluster_assignments.flatten()\n",
    "    df.loc[valid_indices, distance_col_name] = distances.flatten()\n",
    "\n",
    "    return df, kmeans\n",
    "\n",
    "# Example usage with a specified number of clusters\n",
    "n_clusters = 100\n",
    "\n",
    "# Assuming df_sentences and df_paragraph are your DataFrames and they are properly initialized\n",
    "#df_sentences, index_summary  = cluster_vectors_and_save(df_sentences, 'Summary_vector', n_clusters, 'Summary_Cluster', 'Summary_Distance')\n",
    "df_sentences, cluster_index_sentence  = cluster_vectors_and_save(df_sentences, 'Sentence_vector', n_clusters, 'Sentence_Cluster', 'Sentence_Distance')\n",
    "df_paragraph, cluster_index_paragraph  = cluster_vectors_and_save(df_paragraph, 'Paragraph_vector', n_clusters, 'Paragraph_Cluster', 'Paragraph_Distance')\n",
    "df_paragraph, cluster_index_summary  = cluster_vectors_and_save(df_paragraph, 'Summary_vector', n_clusters, 'Summary_Cluster', 'Summary_Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "34701b00-453d-48e8-9c67-8fa15c9b1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Paragraph',\n",
       "       'Paragraph_vector', 'Summary_vector', 'Paragraph_Cluster',\n",
       "       'Paragraph_Distance', 'Summary_Cluster', 'Summary_Distance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paragraph.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "e5f3e18b-714d-4a36-9cc7-a958f2b680c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                                                                        1\n",
       "ProductId                                                        B001E4KFG0\n",
       "UserId                                                       A3SGXH7AUHU8GW\n",
       "ProfileName                                                      delmartian\n",
       "HelpfulnessNumerator                                                      1\n",
       "HelpfulnessDenominator                                                    1\n",
       "Score                                                                     5\n",
       "Time                                                             1303862400\n",
       "Summary                                               Good Quality Dog Food\n",
       "P_index                                                                 P_0\n",
       "S_sentence_number                                                       S_1\n",
       "Sentence                  I have bought several of the Vitality canned d...\n",
       "Summary_vector            [0.10738717, 0.015432298, -0.28650856, 0.00476...\n",
       "Sentence_vector           [0.065722205, 0.2194421, 0.05846874, 0.0385813...\n",
       "Sentence_Cluster                                                       20.0\n",
       "Sentence_Distance                                                  16.56443\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0edaea-cc1a-4276-9feb-fda0a7f52ef1",
   "metadata": {},
   "source": [
    "# good stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "d6490dd4-c3c6-43fb-b8c3-b0c8c10018fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_faiss_index_old(vectors):\n",
    "    dimension = vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(vectors)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "08749008-9406-4df8-a3e2-687fdf6e311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Start time\n",
    "start_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "6ad1f43f-2559-4676-9d08-866e594e60c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7322b46-1b07-4949-a8d7-1c1fbaa043d9",
   "metadata": {},
   "source": [
    "# sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "7c6fdf1a-1216-4744-a633-98694b678689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an important processing step that is required for processing the sentence vector into faiss indexes\n",
    "# sentence_vector\n",
    "Sentence_vector_list = np.array(df_sentences['Sentence_vector'].tolist()).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "a8828aa5-7085-4cac-a56e-f360732c39ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_sentence = create_faiss_index_old(Sentence_vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "8577cdb5-20ee-46d5-9db1-7e506bdc4135",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_sentence_filename = os.path.join(download_dir,\"amazon_reviews_pickle_sentences_similarity_20240219_index_sentence.faiss\")\n",
    "\n",
    "faiss.write_index(index_sentence, index_sentence_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "ff83d481-6600-44d0-b9d2-001904d1dc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of nearest neighbors:  [      0 1648290 1457255 1105728   11931  194732 2270483   27323 1564854\n",
      "  500702  214940 2357430 1699082  934162 2413615  490504   24406 2235628\n",
      "  986503 1004141 1834620 2704142  171935 2681032 2609400 1236060 1563049\n",
      " 2659349 1010907 1341792 1096364  910505  417076  484536 1390625 1976488\n",
      " 1062097  632549 2039959 2765030 1923011  856326 1399801 2804320 1364129\n",
      " 1058201  250729 2539702  479771 2437130  642371 2238572 1378245 2793435\n",
      " 2581837 1414707 1662265  387037 2131983 2468596  476414 2511871  962070\n",
      " 1892188 2266993  539868 1811553 1072813 2379517  563650  866931  640678\n",
      "  822924  654852 2308226 2392336 1480170 2636107 2708434 1418075 2708595\n",
      "  886490 1903565 2119827  898131  590794  614089  794040 2378514  543504\n",
      "  619889 2274346  505114 1374299 1832213 2391859 2758004 2343687 1565404\n",
      "  424864]\n",
      "Distances:  [ 0.       12.972076 14.787857 15.135315 16.024734 16.294922 16.75058\n",
      " 17.21521  17.378815 17.460266 17.718002 17.730347 17.816193 18.167877\n",
      " 18.305283 18.313553 18.63002  18.844467 18.894897 18.894897 18.894897\n",
      " 18.894897 18.894897 18.894897 18.894897 18.894897 18.894897 18.894897\n",
      " 18.917892 18.95642  19.195175 19.217072 19.217072 19.347977 19.477692\n",
      " 19.54039  19.589844 19.652626 19.75145  19.800476 19.81369  19.923279\n",
      " 19.982712 20.016037 20.018494 20.03749  20.265106 20.268997 20.368134\n",
      " 20.410309 20.429138 20.455132 20.455132 20.492401 20.505234 20.505936\n",
      " 20.505936 20.566666 20.606796 20.645508 20.656708 20.658356 20.67276\n",
      " 20.689621 20.689621 20.689621 20.689621 20.712067 20.733429 20.759644\n",
      " 20.761078 20.76448  20.771515 20.803696 20.805466 20.854752 20.873062\n",
      " 20.87558  20.87558  20.87558  20.927185 20.94342  20.945389 20.945389\n",
      " 20.945389 20.952835 20.958786 20.965836 20.973282 21.003448 21.010635\n",
      " 21.010635 21.013397 21.020828 21.029785 21.030212 21.03598  21.042313\n",
      " 21.060379 21.07222 ]\n",
      "The block of code took 22.766013 seconds to execute.\n",
      "the projected duration of the entire vector is 64490.47 seconds - 1074.84 minutes - 17.91 hours to execute.\n"
     ]
    }
   ],
   "source": [
    "# this is expected to take around 36 seconds\n",
    "start_time = time.perf_counter()\n",
    "query_vectors = Sentence_vector_list[:1000] \n",
    "D, I = index_sentence.search(query_vectors, 100)\n",
    "print(\"Indices of nearest neighbors: \", I[0])\n",
    "print(\"Distances: \", D[0])\n",
    "end_time = time.perf_counter()\n",
    "duration = end_time - start_time\n",
    "print(f\"The block of code took {duration:.6f} seconds to execute.\")\n",
    "projected_duration = duration * Sentence_vector_list.shape[0] / 1000\n",
    "projected_minutes = projected_duration / 60\n",
    "projected_hours = projected_minutes / 60\n",
    "print(f\"the projected duration of the entire vector is {projected_duration:.2f} seconds - {projected_minutes:.2f} minutes - {projected_hours:.2f} hours to execute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "dc9162e2-a4cf-4661-b826-563393bfb637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'P_index',\n",
       "       'S_sentence_number', 'Sentence', 'Summary_vector', 'Sentence_vector',\n",
       "       'Sentence_Cluster', 'Sentence_Distance', 'Sentence_similarity_distance',\n",
       "       'Sentence_similarity_index'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "f552e73f-dce5-4503-a560-8a6dd25de5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The block of code took 59332.142764 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "# this is expected to take around 20.55 hours\n",
    "start_time = time.perf_counter()\n",
    "D, I = index_sentence.search(Sentence_vector_list, 100)\n",
    "# Convert D and I to lists and add them as new columns\n",
    "df_sentences['Sentence_similarity_distance'] = list(D)\n",
    "df_sentences['Sentence_similarity_index'] = list(I)\n",
    "end_time = time.perf_counter()\n",
    "duration = end_time - start_time\n",
    "print(f\"The block of code took {duration:.6f} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "87036646-c4bc-43fb-b6ef-13e6951f760c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the projected duration of the entire vector is 33725991.91 seconds - 562099.87 minutes - 9368.33 hours to execute.\n"
     ]
    }
   ],
   "source": [
    "projected_duration = duration * Paragraph_vector_list.shape[0] / 1000\n",
    "projected_minutes = projected_duration / 60\n",
    "projected_hours = projected_minutes / 60\n",
    "print(f\"the projected duration of the entire vector is {projected_duration:.2f} seconds - {projected_minutes:.2f} minutes - {projected_hours:.2f} hours to execute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "2ed28464-ca20-44ac-9a9d-0c3470ec9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert D and I to lists and add them as new columns\n",
    "df_sentences['Sentence_similarity_distance'] = list(D)\n",
    "df_sentences['Sentence_similarity_index'] = list(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "bd88f45d-c213-43f4-a72c-ba59e34c19c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\downloads\\amazon_customer_reviews\\amazon_reviews_pickle_sentences_similarity_20240219.pkl\n"
     ]
    }
   ],
   "source": [
    "df_pickle_filename = os.path.join(download_dir,\"amazon_reviews_pickle_sentences_similarity_20240219.pkl\")\n",
    "df_sentences.to_pickle(df_pickle_filename)\n",
    "print(df_pickle_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb883e5-4147-4959-a8b4-c45413147eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885004b0-1161-49bf-b4ae-99c9d12765f5",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "b6da4d20-0ef7-44a9-9bb7-e81520ad88aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_vector\n",
    "Summary_vector_list = np.array(df_paragraph['Summary_vector'].tolist()).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "605f718f-527f-4dac-8818-0524b4be2dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_summary = create_faiss_index_old(Summary_vector_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "f1d47063-c408-406a-be19-933d79f6b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_summary_filename = os.path.join(download_dir,\"amazon_reviews_pickle_sentences_similarity_20240219_index_summary.faiss\")\n",
    "\n",
    "faiss.write_index(index_summary, index_summary_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "574305a2-e2b0-4d25-9743-814deb1a134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of nearest neighbors:  [382666      0 399359 489673  53168 221484  24587 144559  58892 179267\n",
      " 489807 179559 237222 249997 308826 156565 130790 256872 257297 140106\n",
      " 246845 123858  53993 240237 546826 280248  80214 292611 197335 481025\n",
      " 425084 357520 272799 471405 408685 399362 389326 457292 435098  24730\n",
      "  43956 534734 431616  17680 546814   4741 478079 351362  13702  93694\n",
      "  53981 390602 221514 112837 390672 123846 240116  58922 534102 113382\n",
      " 144589 506154 273688 234223 489837  24617 280236 460904 471580 292599\n",
      " 533922 328765  59685 375852 272206  59694  46024 173265  73176 513559\n",
      "  80202 433171 179297 202140   1587 431634 429104 402906  89779 447699\n",
      " 173250     95 457375 444982 216132 240075 221538 216046 268057 255458]\n",
      "Distances:  [ 0.         0.         3.1913757  3.4827728  3.4827728  4.535492\n",
      "  4.535492   4.535492   4.535492   4.535492   4.535492   4.535492\n",
      "  4.535492   4.6536713  4.6536713  4.6536713  4.6536713  4.6536713\n",
      "  4.6536713  4.6536713  4.6536713  4.732071   4.732071   4.732071\n",
      "  4.732071   4.732071   4.732071   4.732086   7.308243   8.115768\n",
      "  8.115768   8.115784   8.115784   8.28006    8.28006    8.509232\n",
      "  9.400925   9.421875   9.421875   9.421875   9.955185   9.994202\n",
      " 10.049683  10.142471  11.327225  11.327225  11.327225  11.327225\n",
      " 11.327225  11.327225  11.3272705 11.3272705 11.3272705 11.3272705\n",
      " 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705\n",
      " 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705\n",
      " 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705\n",
      " 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705\n",
      " 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705\n",
      " 11.3272705 11.3272705 11.3272705 11.3272705 11.407501  11.425232\n",
      " 11.780258  11.780258  11.798706  12.002029  12.140228  12.140228\n",
      " 12.140228  12.140228  12.140228  12.140228 ]\n",
      "The block of code took 5.161754 seconds to execute.\n",
      "the projected duration of the entire vector is 2934.08 seconds - 48.90 minutes - 0.82 hours to execute.\n"
     ]
    }
   ],
   "source": [
    "# this is expected to take around 36 seconds\n",
    "start_time = time.perf_counter()\n",
    "query_vectors = Summary_vector_list[:1000] \n",
    "D, I = index_summary.search(query_vectors, 100)\n",
    "print(\"Indices of nearest neighbors: \", I[0])\n",
    "print(\"Distances: \", D[0])\n",
    "end_time = time.perf_counter()\n",
    "duration = end_time - start_time\n",
    "print(f\"The block of code took {duration:.6f} seconds to execute.\")\n",
    "projected_duration = duration * Paragraph_vector_list.shape[0] / 1000\n",
    "projected_minutes = projected_duration / 60\n",
    "projected_hours = projected_minutes / 60\n",
    "print(f\"the projected duration of the entire vector is {projected_duration:.2f} seconds - {projected_minutes:.2f} minutes - {projected_hours:.2f} hours to execute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "b589b9e6-7413-486d-9512-604192d99a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of nearest neighbors:  [382666      0 399359 489673  53168 221484  24587 144559  58892 179267\n",
      " 489807 179559 237222 249997 308826 156565 130790 256872 257297 140106\n",
      " 246845 123858  53993 240237 546826 280248  80214 292611 197335 481025\n",
      " 425084 357520 272799 471405 408685 399362 389326 457292 435098  24730\n",
      "  43956 534734 431616  17680 546814   4741 478079 351362  13702  93694\n",
      "  53981 390602 221514 112837 390672 123846 240116  58922 534102 113382\n",
      " 144589 506154 273688 234223 489837  24617 280236 460904 471580 292599\n",
      " 533922 328765  59685 375852 272206  59694  46024 173265  73176 513559\n",
      "  80202 433171 179297 202140   1587 431634 429104 402906  89779 447699\n",
      " 173250     95 457375 444982 216132 240075 221538 216046 268057 255458]\n",
      "Distances:  [ 0.         0.         3.1913757  3.4827728  3.4827728  4.535492\n",
      "  4.535492   4.535492   4.535492   4.535492   4.535492   4.535492\n",
      "  4.535492   4.6536713  4.6536713  4.6536713  4.6536713  4.6536713\n",
      "  4.6536713  4.6536713  4.6536713  4.732071   4.732071   4.732071\n",
      "  4.732071   4.732071   4.732071   4.732086   7.308243   8.115768\n",
      "  8.115768   8.115784   8.115784   8.28006    8.28006    8.509232\n",
      "  9.400925   9.421875   9.421875   9.421875   9.955185   9.994202\n",
      " 10.049683  10.142471  11.327225  11.327225  11.327225  11.327225\n",
      " 11.327225  11.327225  11.3272705 11.3272705 11.3272705 11.3272705\n",
      " 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705\n",
      " 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705\n",
      " 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705\n",
      " 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705\n",
      " 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705 11.3272705\n",
      " 11.3272705 11.3272705 11.3272705 11.3272705 11.407501  11.425232\n",
      " 11.780258  11.780258  11.798706  12.002029  12.140228  12.140228\n",
      " 12.140228  12.140228  12.140228  12.140228 ]\n",
      "The block of code took 2472.287279 seconds to execute.\n",
      "The block of code took 2472.29 seconds - 41.20 minutes - 0.69 hours to execute.\n"
     ]
    }
   ],
   "source": [
    "# this is expected to take around 36 seconds\n",
    "start_time = time.perf_counter()\n",
    "query_vectors = Summary_vector_list\n",
    "D, I = index_summary.search(query_vectors, 100)\n",
    "print(\"Indices of nearest neighbors: \", I[0])\n",
    "print(\"Distances: \", D[0])\n",
    "end_time = time.perf_counter()\n",
    "duration = end_time - start_time\n",
    "print(f\"The block of code took {duration:.6f} seconds to execute.\")\n",
    "Seconds_duration = duration \n",
    "projected_minutes = Seconds_duration / 60\n",
    "projected_hours = projected_minutes / 60\n",
    "print(f\"The block of code took {Seconds_duration:.2f} seconds - {projected_minutes:.2f} minutes - {projected_hours:.2f} hours to execute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "89307c84-48fb-48dd-80c0-11a0d3182c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert D and I to lists and add them as new columns\n",
    "df_paragraph['Summary_similarity_distance'] = list(D)\n",
    "df_paragraph['Summary_similarity_index'] = list(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "5f3ad054-f309-4688-aab7-f8b0257beaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                                                                               1\n",
       "ProductId                                                               B001E4KFG0\n",
       "UserId                                                              A3SGXH7AUHU8GW\n",
       "ProfileName                                                             delmartian\n",
       "HelpfulnessNumerator                                                             1\n",
       "HelpfulnessDenominator                                                           1\n",
       "Score                                                                            5\n",
       "Time                                                                    1303862400\n",
       "Summary                                                      Good Quality Dog Food\n",
       "Paragraph                        I have bought several of the Vitality canned d...\n",
       "Paragraph_vector                 [-0.12606543, 0.062131397, 0.0551221, -0.00242...\n",
       "Summary_vector                   [0.10738717, 0.015432298, -0.28650856, 0.00476...\n",
       "Paragraph_Cluster                                                             97.0\n",
       "Paragraph_Distance                                                        8.618866\n",
       "Summary_Cluster                                                               72.0\n",
       "Summary_Distance                                                         18.906586\n",
       "Summary_similarity_distance      [0.0, 0.0, 3.1913757, 3.4827728, 3.4827728, 4....\n",
       "Summary_similarity_index         [382666, 0, 399359, 489673, 53168, 221484, 245...\n",
       "Paragraph_similarity_distance    [0.0, 9.213089, 9.347366, 9.460304, 9.700714, ...\n",
       "Paragraph_similarity_index       [0, 357080, 399351, 118925, 87038, 174270, 779...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paragraph.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f7a572-cb40-4f94-9b89-bb528d660c46",
   "metadata": {},
   "source": [
    "# paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "fcb5d4f1-24ff-494c-8147-fc88dfd1a335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568427  length of paragraph_vector_list\n"
     ]
    }
   ],
   "source": [
    "# summary_vector\n",
    "Paragraph_vector_list = np.array(df_paragraph['Paragraph_vector'].tolist()).astype('float32')\n",
    "print(Paragraph_vector_list.shape[0], \" length of paragraph_vector_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "ef466779-0d5b-4baf-9358-fe5f36823d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_paragraph = create_faiss_index_old(Paragraph_vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "9d3bc121-2774-463a-9dfe-5dd255710530",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_paragraph_filename = os.path.join(download_dir,\"amazon_reviews_pickle_sentences_similarity_20240219_index_paragraph.faiss\")\n",
    "\n",
    "faiss.write_index(index_paragraph, index_paragraph_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "896c7a2e-7d09-47c7-9bda-8e9a1903615e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of nearest neighbors:  [     0 357080 399351 118925  87038 174270  77939 524385 506600 509370\n",
      " 238877 198732 407622  77238 322662 112837  89760 177785 208743 119607\n",
      " 242786 341089 339590 276987 285705 508415  47652 148269 139618 539207\n",
      " 184475  20291 116865 442312 356994 171393 115526 277988 146493 410231\n",
      " 343243 486079 191541 437078 211242 444976 509258 399177  50954 383008\n",
      " 306378 169618 414238 505381 110205 271600  18318 195419   8264 447699\n",
      " 560668  41860 253063 359988 146781  80964 253565 512306 162591 273712\n",
      " 322274 411187  84855 525083 386033 241405 295356 439358 160889 510108\n",
      " 379110  54435 555674 210219 272272 514112 169374 475930 325901 240225\n",
      "  14205 309251 213124 426659 292697  83226 160240 401848 300072 315123]\n",
      "Distances:  [ 0.         9.213089   9.347366   9.460304   9.700714   9.70076\n",
      "  9.70076    9.70076    9.70076    9.70076    9.70076    9.70076\n",
      "  9.70076    9.70076    9.701889  10.113693  10.255371  10.263527\n",
      " 10.2729645 10.301964  10.332123  10.355682  10.420395  10.544197\n",
      " 10.554474  10.554474  10.554474  10.575752  10.576309  10.576309\n",
      " 10.576309  10.576309  10.576324  10.579697  10.589935  10.589935\n",
      " 10.589943  10.589943  10.589943  10.589943  10.589943  10.589943\n",
      " 10.589943  10.589943  10.589943  10.630844  10.635422  10.635422\n",
      " 10.635422  10.635422  10.635422  10.635422  10.635422  10.635422\n",
      " 10.635437  10.6716    10.676559  10.694992  10.694992  10.699684\n",
      " 10.753281  10.765335  10.767212  10.7866745 10.787247  10.789093\n",
      " 10.81221   10.820984  10.820984  10.858795  10.868179  10.89344\n",
      " 10.904137  10.927902  10.933929  10.941132  10.958763  10.962982\n",
      " 10.975288  10.994675  11.003754  11.003754  11.003754  11.003754\n",
      " 11.021919  11.021919  11.021919  11.021919  11.0252    11.065582\n",
      " 11.066528  11.067139  11.067139  11.067169  11.067169  11.067169\n",
      " 11.067169  11.067169  11.067169  11.067169 ]\n",
      "The block of code took 4.635138 seconds to execute.\n",
      "the projected duration of the entire vector is 2634.74 seconds - 43.91 minutes - 0.73 hours to execute.\n"
     ]
    }
   ],
   "source": [
    "# this is expected to take around 36 seconds\n",
    "start_time = time.perf_counter()\n",
    "query_vectors = Paragraph_vector_list[:1000] \n",
    "D, I = index_paragraph.search(query_vectors, 100)\n",
    "print(\"Indices of nearest neighbors: \", I[0])\n",
    "print(\"Distances: \", D[0])\n",
    "end_time = time.perf_counter()\n",
    "duration = end_time - start_time\n",
    "print(f\"The block of code took {duration:.6f} seconds to execute.\")\n",
    "projected_duration = duration * Paragraph_vector_list.shape[0] / 1000\n",
    "projected_minutes = projected_duration / 60\n",
    "projected_hours = projected_minutes / 60\n",
    "print(f\"the projected duration of the entire vector is {projected_duration:.2f} seconds - {projected_minutes:.2f} minutes - {projected_hours:.2f} hours to execute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "dd2fa13d-70aa-4a0d-b240-448536934d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of nearest neighbors:  [     0 357080 399351 118925  87038 174270  77939 524385 506600 509370\n",
      " 238877 198732 407622  77238 322662 112837  89760 177785 208743 119607\n",
      " 242786 341089 339590 276987 285705 508415  47652 148269 139618 539207\n",
      " 184475  20291 116865 442312 356994 171393 115526 277988 146493 410231\n",
      " 343243 486079 191541 437078 211242 444976 509258 399177  50954 383008\n",
      " 306378 169618 414238 505381 110205 271600  18318 195419   8264 447699\n",
      " 560668  41860 253063 359988 146781  80964 253565 512306 162591 273712\n",
      " 322274 411187  84855 525083 386033 241405 295356 439358 160889 510108\n",
      " 379110  54435 555674 210219 272272 514112 169374 475930 325901 240225\n",
      "  14205 309251 213124 426659 292697  83226 160240 401848 300072 315123]\n",
      "Distances:  [ 0.         9.213089   9.347366   9.460304   9.700714   9.70076\n",
      "  9.70076    9.70076    9.70076    9.70076    9.70076    9.70076\n",
      "  9.70076    9.70076    9.701889  10.113693  10.255371  10.263527\n",
      " 10.2729645 10.301964  10.332123  10.355682  10.420395  10.544197\n",
      " 10.554474  10.554474  10.554474  10.575752  10.576309  10.576309\n",
      " 10.576309  10.576309  10.576324  10.579697  10.589935  10.589935\n",
      " 10.589943  10.589943  10.589943  10.589943  10.589943  10.589943\n",
      " 10.589943  10.589943  10.589943  10.630844  10.635422  10.635422\n",
      " 10.635422  10.635422  10.635422  10.635422  10.635422  10.635422\n",
      " 10.635437  10.6716    10.676559  10.694992  10.694992  10.699684\n",
      " 10.753281  10.765335  10.767212  10.7866745 10.787247  10.789093\n",
      " 10.81221   10.820984  10.820984  10.858795  10.868179  10.89344\n",
      " 10.904137  10.927902  10.933929  10.941132  10.958763  10.962982\n",
      " 10.975288  10.994675  11.003754  11.003754  11.003754  11.003754\n",
      " 11.021919  11.021919  11.021919  11.021919  11.0252    11.065582\n",
      " 11.066528  11.067139  11.067139  11.067169  11.067169  11.067169\n",
      " 11.067169  11.067169  11.067169  11.067169 ]\n",
      "The block of code took 2395.946036 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "# this is expected to take around 48.92 minutes\n",
    "start_time = time.perf_counter()\n",
    "query_vectors = Paragraph_vector_list\n",
    "D, I = index_paragraph.search(query_vectors, 100)\n",
    "print(\"Indices of nearest neighbors: \", I[0])\n",
    "print(\"Distances: \", D[0])\n",
    "end_time = time.perf_counter()\n",
    "duration = end_time - start_time\n",
    "print(f\"The block of code took {duration:.6f} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "87e01ae7-a320-4fc7-a71a-e29fdbb15a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568427"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "6e8a5877-3f20-48fa-a2d4-7b665f112fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert D and I to lists and add them as new columns\n",
    "df_paragraph['Paragraph_similarity_distance'] = list(D)\n",
    "df_paragraph['Paragraph_similarity_index'] = list(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "1c8cc826-5157-477f-a446-787c1d8c79e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\downloads\\amazon_customer_reviews\\amazon_reviews_pickle_paragraphs_similarity_20240219.pkl\n"
     ]
    }
   ],
   "source": [
    "df_pickle_filename = os.path.join(download_dir,\"amazon_reviews_pickle_paragraphs_similarity_20240219.pkl\")\n",
    "df_paragraph.to_pickle(df_pickle_filename)\n",
    "print(df_pickle_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "f1213371-2f5e-4b05-b660-72e9dd2b8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Paragraph_vector', 'Summary_vector']\n",
    "df_paragraph_reduced = df_paragraph.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "a1f8508a-fe32-4a2c-b07f-3ed2f19d035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\downloads\\amazon_customer_reviews\\amazon_reviews_pickle_paragraphs_similarity_reduced_20240219.pkl\n"
     ]
    }
   ],
   "source": [
    "df_pickle_reduced_filename = os.path.join(download_dir,\"amazon_reviews_pickle_paragraphs_similarity_reduced_20240219.pkl\")\n",
    "df_paragraph_reduced.to_pickle(df_pickle_reduced_filename)\n",
    "print(df_pickle_reduced_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed0736-2443-4cdb-bc19-b06eaabe19af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "fbde22f3-a4b5-49e8-9883-f2d05cb09c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Paragraph',\n",
       "       'Paragraph_vector', 'Summary_vector', 'Paragraph_Cluster',\n",
       "       'Paragraph_Distance', 'Summary_Cluster', 'Summary_Distance',\n",
       "       'Summary_similarity_distance', 'Summary_similarity_index',\n",
       "       'Paragraph_similarity_distance', 'Paragraph_similarity_index'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paragraph.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "e7d2526d-2eb8-45fb-8ee9-c5cbf7f91c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paragraph.iloc[0]['Paragraph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "1ed9eccc-a113-4694-a337-8bf4883331d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.       ,  0.       ,  3.1913757,  3.4827728,  3.4827728,\n",
       "        4.535492 ,  4.535492 ,  4.535492 ,  4.535492 ,  4.535492 ,\n",
       "        4.535492 ,  4.535492 ,  4.535492 ,  4.6536713,  4.6536713,\n",
       "        4.6536713,  4.6536713,  4.6536713,  4.6536713,  4.6536713,\n",
       "        4.6536713,  4.732071 ,  4.732071 ,  4.732071 ,  4.732071 ,\n",
       "        4.732071 ,  4.732071 ,  4.732086 ,  7.308243 ,  8.115768 ,\n",
       "        8.115768 ,  8.115784 ,  8.115784 ,  8.28006  ,  8.28006  ,\n",
       "        8.509232 ,  9.400925 ,  9.421875 ,  9.421875 ,  9.421875 ,\n",
       "        9.955185 ,  9.994202 , 10.049683 , 10.142471 , 11.327225 ,\n",
       "       11.327225 , 11.327225 , 11.327225 , 11.327225 , 11.327225 ,\n",
       "       11.3272705, 11.3272705, 11.3272705, 11.3272705, 11.3272705,\n",
       "       11.3272705, 11.3272705, 11.3272705, 11.3272705, 11.3272705,\n",
       "       11.3272705, 11.3272705, 11.3272705, 11.3272705, 11.3272705,\n",
       "       11.3272705, 11.3272705, 11.3272705, 11.3272705, 11.3272705,\n",
       "       11.3272705, 11.3272705, 11.3272705, 11.3272705, 11.3272705,\n",
       "       11.3272705, 11.3272705, 11.3272705, 11.3272705, 11.3272705,\n",
       "       11.3272705, 11.3272705, 11.3272705, 11.3272705, 11.3272705,\n",
       "       11.3272705, 11.3272705, 11.3272705, 11.407501 , 11.425232 ,\n",
       "       11.780258 , 11.780258 , 11.798706 , 12.002029 , 12.140228 ,\n",
       "       12.140228 , 12.140228 , 12.140228 , 12.140228 , 12.140228 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paragraph.iloc[0]['Summary_similarity_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "6e3519cf-7b29-4cbb-a4fc-268d68f36b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([382666,      0, 399359, 489673,  53168, 221484,  24587, 144559,\n",
       "        58892, 179267, 489807, 179559, 237222, 249997, 308826, 156565,\n",
       "       130790, 256872, 257297, 140106, 246845, 123858,  53993, 240237,\n",
       "       546826, 280248,  80214, 292611, 197335, 481025, 425084, 357520,\n",
       "       272799, 471405, 408685, 399362, 389326, 457292, 435098,  24730,\n",
       "        43956, 534734, 431616,  17680, 546814,   4741, 478079, 351362,\n",
       "        13702,  93694,  53981, 390602, 221514, 112837, 390672, 123846,\n",
       "       240116,  58922, 534102, 113382, 144589, 506154, 273688, 234223,\n",
       "       489837,  24617, 280236, 460904, 471580, 292599, 533922, 328765,\n",
       "        59685, 375852, 272206,  59694,  46024, 173265,  73176, 513559,\n",
       "        80202, 433171, 179297, 202140,   1587, 431634, 429104, 402906,\n",
       "        89779, 447699, 173250,     95, 457375, 444982, 216132, 240075,\n",
       "       221538, 216046, 268057, 255458], dtype=int64)"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paragraph.iloc[0]['Summary_similarity_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c07616-8e53-4241-b169-390e53c1941a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "d725204b-a787-462d-b951-0fec068595c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductId\n",
       "B007JFMH8M    913\n",
       "B0026RQTGE    632\n",
       "B002QWHJOU    632\n",
       "B002QWP89S    632\n",
       "B002QWP8H0    632\n",
       "             ... \n",
       "B004DSPTTM      1\n",
       "B008C9QWU8      1\n",
       "B007O5A6BM      1\n",
       "B003Q4TZ08      1\n",
       "B001LR2CU2      1\n",
       "Name: count, Length: 74258, dtype: int64"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paragraph['ProductId'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "489c5839-120e-416e-a78e-4c40df9a8029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "[1, 2)         30408\n",
      "[2, 5)         23435\n",
      "[5, 10)         9796\n",
      "[10, 50)        8775\n",
      "[50, 100)        978\n",
      "[100, 1000)      866\n",
      "Name: count, dtype: int64\n",
      "74258\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming 'value_counts_series' is your Series object from the 'value_counts()' operation\n",
    "value_counts_series = df_paragraph['ProductId'].value_counts()\n",
    "\n",
    "# Define bins for the range of counts you're interested in. Adjust these as needed.\n",
    "bins = [1,2,5, 10, 50, 100,  1000]\n",
    "\n",
    "# Use pd.cut() to bin the value counts into the specified ranges\n",
    "binned_counts = pd.cut(value_counts_series, bins, right=False)\n",
    "\n",
    "# Count the number of product IDs in each bin\n",
    "binned_counts_summary = binned_counts.value_counts().sort_index()\n",
    "\n",
    "print(binned_counts_summary)\n",
    "print(binned_counts_summary.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "2b535062-6743-433e-857d-0bddbde9c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score\n",
    "def columns_bins(column_name):\n",
    "    \n",
    "    # Assuming 'value_counts_series' is your Series object from the 'value_counts()' operation\n",
    "    value_counts_series = df_paragraph[column_name].value_counts()\n",
    "    print(value_counts_series)\n",
    "    # Define bins for the range of counts you're interested in. Adjust these as needed.\n",
    "    bins = [1,2,5, 10, 50, 100,  1000]\n",
    "    \n",
    "    # Use pd.cut() to bin the value counts into the specified ranges\n",
    "    binned_counts = pd.cut(value_counts_series, bins, right=False)\n",
    "    \n",
    "    # Count the number of product IDs in each bin\n",
    "    binned_counts_summary = binned_counts.value_counts().sort_index()\n",
    "    \n",
    "    print(binned_counts_summary)\n",
    "    return(binned_counts_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "c15d67b6-b32d-4a52-bf9d-7034f4946fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score\n",
      "5    363122\n",
      "4     80655\n",
      "1     52268\n",
      "3     42638\n",
      "2     29744\n",
      "Name: count, dtype: int64\n",
      "count\n",
      "[1, 2)         0\n",
      "[2, 5)         0\n",
      "[5, 10)        0\n",
      "[10, 50)       0\n",
      "[50, 100)      0\n",
      "[100, 1000)    0\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "my_bin_data = columns_bins('Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "08040b6d-e195-4c39-9cab-514a679da39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43850"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binned_counts_summary.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "ae7c3a26-a15b-4f0b-ac7b-03e1b173e9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score            1     2      3      4       5\n",
      "Bin                                           \n",
      "[1, 2)        3215  1580   2196   3909   19508\n",
      "[2, 5)        6704  3251   4104   7668   40640\n",
      "[5, 10)       6233  3305   4033   8041   42314\n",
      "[10, 50)     15696  8449  11509  22145  110841\n",
      "[50, 100)     5672  3270   4927   9753   43464\n",
      "[100, 1000)  14748  9889  15869  29139  106355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnny\\AppData\\Local\\Temp\\ipykernel_4536\\1979155073.py:25: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  final_distribution = score_distribution.groupby(['Bin', 'Score'])['ScoreCount'].sum().unstack(fill_value=0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame creation (replace this with your actual DataFrame)\n",
    "# df = ...\n",
    "\n",
    "# Step 1: Calculate the frequency of each ProductId\n",
    "product_counts = df_paragraph['ProductId'].value_counts().reset_index()\n",
    "product_counts.columns = ['ProductId', 'Count']\n",
    "\n",
    "# Step 2: Bin these frequencies\n",
    "bins = [1, 2, 5, 10, 50, 100, 1000]\n",
    "product_counts['Bin'] = pd.cut(product_counts['Count'], bins, right=False)\n",
    "\n",
    "# Merge the binned counts back to the original DataFrame to associate each row with a bin\n",
    "df = df_paragraph.merge(product_counts[['ProductId', 'Bin']], on='ProductId')\n",
    "\n",
    "# Step 3 & 4: For each ProductId, calculate the distribution of scores, then aggregate by bins\n",
    "# First, group by both ProductId and Score to get the count of each score for each ProductId\n",
    "score_distribution = df.groupby(['ProductId', 'Score']).size().reset_index(name='ScoreCount')\n",
    "\n",
    "# Merge this distribution back with the bins\n",
    "score_distribution = score_distribution.merge(product_counts[['ProductId', 'Bin']], on='ProductId')\n",
    "\n",
    "# Now, aggregate this information by bin and score\n",
    "final_distribution = score_distribution.groupby(['Bin', 'Score'])['ScoreCount'].sum().unstack(fill_value=0)\n",
    "\n",
    "print(final_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "67d431b8-474d-4d19-b662-a20cde8768c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score            1     2      3      4       5  Subtotal\n",
      "Bin                                                     \n",
      "[1, 2)        3215  1580   2196   3909   19508     30408\n",
      "[2, 5)        6704  3251   4104   7668   40640     62367\n",
      "[5, 10)       6233  3305   4033   8041   42314     63926\n",
      "[10, 50)     15696  8449  11509  22145  110841    168640\n",
      "[50, 100)     5672  3270   4927   9753   43464     67086\n",
      "[100, 1000)  14748  9889  15869  29139  106355    176000\n",
      "Grand Total: 568427\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'final_distribution' is the DataFrame you've obtained from the previous steps\n",
    "\n",
    "# Sum the rows to get subtotals for each bin\n",
    "final_distribution['Subtotal'] = final_distribution.sum(axis=1)\n",
    "\n",
    "# Display the updated DataFrame with subtotals\n",
    "print(final_distribution)\n",
    "\n",
    "# Sum the subtotals to get the grand total\n",
    "grand_total = final_distribution['Subtotal'].sum()\n",
    "\n",
    "print(f\"Grand Total: {grand_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7cf0cf8-0e3d-4574-81c1-d214268f4df5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_paragraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_paragraph\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProductId\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_paragraph' is not defined"
     ]
    }
   ],
   "source": [
    "df_paragraph['ProductId'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0774e5-cac6-4ba5-9bb8-709806bb3077",
   "metadata": {},
   "source": [
    "# stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "92f22e74-b447-4db8-8020-74964c9f6f19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[584], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b2d5d1-c253-4328-be9e-ed23175fd947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aecef3-e139-4a9d-91e6-b9707fddde6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this demonstrates making distance and index of a query\n",
    "# k is the number of similarities to provide\n",
    "# # this example is looking for the top 20 similar sentences to the first sentence in the df\n",
    "start_time = time.perf_counter()\n",
    "# Create a random query vector (or a set of vectors)\n",
    "# dim was for the kmeans clusters\n",
    "# dim = index.d  # Dimension of the vectors in the index\n",
    "query_vectors = Sentence_vector_list[0].reshape(1, -1) # this reshape is required if passing in a single vector\n",
    "\n",
    "# Perform the search\n",
    "k = 20  # Number of nearest neighbors to retrieve\n",
    "D, I = index.search(query_vectors, k)  # D: distances, I: indices of the neighbors\n",
    "end_time = time.perf_counter()\n",
    "duration = end_time - start_time\n",
    "print(f\"The block of code took {duration:.6f} seconds to execute.\")\n",
    "print(\"Indices of nearest neighbors: \", I)\n",
    "print(\"Distances: \", D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5f6b5-49bb-44b7-b0f5-0c9bdfecb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this example code runs for 105 seconds and processes 5000 vectors\n",
    "start_time = time.perf_counter()\n",
    "# Select the first 10 vectors from Sentence_vector_list\n",
    "query_vectors = Sentence_vector_list[:1000] # same as the above example but this is provided multiple vectors as search vectors\n",
    "\n",
    "# Search for the 5 closest vectors to each of the 10 query vectors\n",
    "k = 100  # Number of nearest neighbors to find\n",
    "# there was no appreciable difference in run time for how many k results to give back from 5 to 100\n",
    "# it is possible that the entire similarity could be provided but the storage space cost could be problematic.\n",
    "D, I = index.search(query_vectors, k)  # D: distances, I: indices of the nearest neighbors\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "duration = end_time - start_time\n",
    "print(f\"The block of code took {duration:.6f} seconds to execute.\")\n",
    "print(\"Indices of nearest neighbors: \", I)\n",
    "print(\"Distances: \", D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c0a783-9125-4e1c-96fc-26c8abb95553",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "query_vectors = Sentence_vector_list # same as the above example but this is provided multiple vectors as search vectors\n",
    "\n",
    "# Search for the 5 closest vectors to each of the 10 query vectors\n",
    "k = 100  # Number of nearest neighbors to find\n",
    "\n",
    "D, I = index.search(query_vectors, k)  # D: distances, I: indices of the nearest neighbors\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "duration = end_time - start_time\n",
    "print(f\"The block of code took {duration:.6f} seconds to execute.\")\n",
    "print(\"Indices of nearest neighbors: \", I)\n",
    "print(\"Distances: \", D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56911107-deb4-41f3-9c51-bd0c98b59fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed8e9a8-76ab-48cc-a8f6-a48e07694b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop # stop the code from running the code below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2afebde-849f-410e-9807-9bfd11f945f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# untested concurrent code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff144a8-56b3-4387-a042-7846c0eb34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame and it contains a column 'vector' with your query vectors\n",
    "query_vectors = df['vector'].tolist()  # Convert the DataFrame column to a list of vectors\n",
    "\n",
    "# Define a function to perform the search for a single vector\n",
    "def search_vector(vector):\n",
    "    D, I = index.search(vector.reshape(1, -1), k)  # Reshape for single query compatibility\n",
    "    return D, I\n",
    "\n",
    "# Perform parallel search\n",
    "def parallel_search(query_vectors, num_workers=4):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        # Submit all the search tasks at once and return futures\n",
    "        future_to_vector = {executor.submit(search_vector, vector): vector for vector in query_vectors}\n",
    "        \n",
    "        # As each future completes, process its result\n",
    "        for future in as_completed(future_to_vector):\n",
    "            vector = future_to_vector[future]\n",
    "            try:\n",
    "                D, I = future.result()  # Retrieves the result from the future\n",
    "                results.append((D, I))\n",
    "            except Exception as exc:\n",
    "                print(f'Generated an exception: {exc}')\n",
    "    return results\n",
    "\n",
    "# Adjust num_workers based on your machine's capability\n",
    "results = parallel_search(query_vectors, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef85756-decc-4c13-a255-92c82d4f5053",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1099a40-b7b9-45ad-9bda-7b4822f53043",
   "metadata": {},
   "source": [
    "# examining clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a050a6c-86dd-429a-8dc1-68124215fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c63a280-1d3b-480b-8489-f0bdd2f7055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a7e91c-d4ea-4e87-b4ea-aabeab1c8721",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraph.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5748d0e-e48b-4b57-bb59-3a0c3bb34af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraph.iloc[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5cc813-00ca-4b96-b852-fb2448dd025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 12\n",
    "cluster_0_df = df_paragraph[df_paragraph['Paragraph_Cluster'] == cluster]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e52e7-7055-4d4f-a20f-66e42e5ef2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7addd8-36d4-425f-870b-90f636e1bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows in each cluster for paragraph vectors\n",
    "paragraph_cluster_counts = df_paragraph['Paragraph_Cluster'].value_counts().sort_index()\n",
    "#print(\"Paragraph Cluster Distribution:\\n\", paragraph_cluster_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83145315-3f96-4dec-908b-b132cfcd14f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows in each cluster for summary vectors\n",
    "summary_cluster_counts = df_sentences['Summary_Cluster'].value_counts().sort_index()\n",
    "#print(\"Summary Cluster Distribution:\\n\", summary_cluster_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a04666-1962-4275-9b80-612582ddd823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows in each cluster for sentence vectors\n",
    "sentence_cluster_counts = df_sentences['Sentence_Cluster'].value_counts().sort_index()\n",
    "#print(\"Sentence Cluster Distribution:\\n\", sentence_cluster_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a6cff-1225-4a91-a046-af8aa26a31d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the distribution of paragraph clusters\n",
    "paragraph_cluster_counts.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Distribution of Paragraph Clusters')\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('Number of Rows')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3bf423-9517-4743-b316-9407d564dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, print some sample paragraphs to get a feel for the content\n",
    "for paragraph in cluster_0_df['Paragraph'].sample(n=5).values:\n",
    "    print(paragraph, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b184359-2430-44e7-aa0a-6d6c80c892e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_stats = cluster_0_df['Paragraph_Distance'].describe()\n",
    "print(distance_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa7cfc-3dcc-4d72-81a8-15a921d62f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192035cb-5282-43f2-895f-9fad57c3da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Concatenate all paragraphs in the cluster into a single string\n",
    "text = \" \".join(paragraph for paragraph in cluster_0_df['Paragraph'])\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea9ef8-5ead-4548-9c5e-74b65595df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5671c4-d32f-4e86-b2c1-a8dbba18f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(wordcloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d0c783-6e12-400d-ac65-61f51af4c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word frequencies:\", wordcloud.words_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931fa628-8660-49ff-bbe3-947d24d76d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Word layout:\", wordcloud.layout_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06d3a7-085c-444c-ab0f-00bfc28ecf83",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "cee362f5-6d07-4095-baa1-7049bdc84cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0_df = df_paragraph[df_paragraph['Paragraph_Cluster'] == cluster]\n",
    "texts = cluster_0_df['Paragraph'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "875886c1-a7b6-4bc4-a0ac-edff6b592fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\johnny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Ensure you have the necessary NLTK data\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tokenization and stop words removal\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "texts_tokenized = [[word for word in tokenizer.tokenize(text.lower()) if word not in stop_words] for text in texts]\n",
    "\n",
    "# Creating a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(texts_tokenized)\n",
    "\n",
    "# Filtering out extremes to limit the number of features\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "# Creating the document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_tokenized]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81202abf-c606-4c35-9a19-964b4a7a4e68",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "10370dff-ae42-457a-8db2-0f4761453b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.097*\"br\" + 0.023*\"bought\" + 0.020*\"since\" + 0.018*\"got\"')\n",
      "(1, '0.030*\"great\" + 0.027*\"shipping\" + 0.026*\"br\" + 0.021*\"gum\"')\n",
      "(2, '0.103*\"popcorn\" + 0.021*\"save\" + 0.020*\"great\" + 0.019*\"movie\"')\n",
      "(3, '0.023*\"one\" + 0.020*\"red\" + 0.017*\"size\" + 0.016*\"good\"')\n",
      "(4, '0.038*\"syrup\" + 0.020*\"chai\" + 0.017*\"good\" + 0.017*\"maple\"')\n",
      "(5, '0.023*\"find\" + 0.022*\"asked\" + 0.017*\"get\" + 0.016*\"tasted\"')\n",
      "(6, '0.055*\"water\" + 0.050*\"coconut\" + 0.023*\"would\" + 0.018*\"chips\"')\n",
      "(7, '0.017*\"great\" + 0.014*\"like\" + 0.012*\"pretzels\" + 0.012*\"website\"')\n",
      "(8, '0.039*\"years\" + 0.039*\"find\" + 0.032*\"moved\" + 0.026*\"live\"')\n",
      "(9, '0.023*\"like\" + 0.017*\"good\" + 0.015*\"product\" + 0.014*\"amazon\"')\n",
      "(10, '0.035*\"really\" + 0.022*\"product\" + 0.021*\"best\" + 0.015*\"well\"')\n",
      "(11, '0.067*\"find\" + 0.049*\"amazon\" + 0.038*\"stores\" + 0.020*\"found\"')\n",
      "(12, '0.037*\"would\" + 0.026*\"put\" + 0.022*\"think\" + 0.021*\"bag\"')\n",
      "(13, '0.039*\"try\" + 0.034*\"tried\" + 0.029*\"brand\" + 0.024*\"brands\"')\n",
      "(14, '0.019*\"trip\" + 0.018*\"old\" + 0.018*\"stuff\" + 0.016*\"like\"')\n",
      "(15, '0.047*\"cheese\" + 0.021*\"find\" + 0.017*\"italy\" + 0.015*\"ship\"')\n",
      "(16, '0.087*\"chocolate\" + 0.041*\"milk\" + 0.027*\"get\" + 0.023*\"would\"')\n",
      "(17, '0.048*\"salt\" + 0.046*\"seasoning\" + 0.042*\"ever\" + 0.041*\"best\"')\n",
      "(18, '0.212*\"coffee\" + 0.035*\"cup\" + 0.021*\"great\" + 0.021*\"love\"')\n",
      "(19, '0.038*\"cream\" + 0.033*\"like\" + 0.029*\"ice\" + 0.026*\"much\"')\n",
      "(20, '0.065*\"oil\" + 0.039*\"mustard\" + 0.034*\"olive\" + 0.033*\"sugar\"')\n",
      "(21, '0.037*\"soup\" + 0.024*\"breakfast\" + 0.018*\"good\" + 0.016*\"love\"')\n",
      "(22, '0.211*\"tea\" + 0.020*\"one\" + 0.017*\"green\" + 0.017*\"teas\"')\n",
      "(23, '0.045*\"bar\" + 0.041*\"bars\" + 0.032*\"great\" + 0.029*\"snack\"')\n",
      "(24, '0.038*\"love\" + 0.027*\"oatmeal\" + 0.021*\"case\" + 0.021*\"product\"')\n",
      "(25, '0.047*\"recipe\" + 0.032*\"make\" + 0.023*\"jelly\" + 0.022*\"would\"')\n",
      "(26, '0.095*\"candy\" + 0.022*\"buy\" + 0.021*\"ones\" + 0.020*\"candies\"')\n",
      "(27, '0.101*\"mix\" + 0.033*\"made\" + 0.031*\"best\" + 0.023*\"first\"')\n",
      "(28, '0.031*\"get\" + 0.022*\"enough\" + 0.022*\"hit\" + 0.020*\"try\"')\n",
      "(29, '0.053*\"christmas\" + 0.047*\"gift\" + 0.039*\"cookies\" + 0.024*\"loved\"')\n",
      "(30, '0.027*\"5\" + 0.024*\"amazon\" + 0.018*\"bought\" + 0.017*\"best\"')\n",
      "(31, '0.063*\"amazon\" + 0.040*\"found\" + 0.023*\"com\" + 0.023*\"thank\"')\n",
      "(32, '0.055*\"grocery\" + 0.053*\"store\" + 0.047*\"local\" + 0.046*\"amazon\"')\n",
      "(33, '0.030*\"licorice\" + 0.028*\"tried\" + 0.023*\"time\" + 0.020*\"simply\"')\n",
      "(34, '0.057*\"recommend\" + 0.045*\"highly\" + 0.024*\"product\" + 0.022*\"would\"')\n",
      "(35, '0.039*\"bread\" + 0.030*\"food\" + 0.019*\"like\" + 0.018*\"great\"')\n",
      "(36, '0.037*\"remember\" + 0.027*\"good\" + 0.027*\"still\" + 0.025*\"kid\"')\n",
      "(37, '0.048*\"love\" + 0.024*\"mouth\" + 0.022*\"say\" + 0.018*\"one\"')\n",
      "(38, '0.040*\"vanilla\" + 0.026*\"great\" + 0.020*\"good\" + 0.018*\"used\"')\n",
      "(39, '0.057*\"cake\" + 0.029*\"chocolate\" + 0.025*\"everyone\" + 0.025*\"party\"')\n",
      "(40, '0.055*\"beans\" + 0.026*\"wine\" + 0.024*\"site\" + 0.021*\"show\"')\n",
      "(41, '0.044*\"fan\" + 0.029*\"big\" + 0.025*\"huge\" + 0.022*\"sister\"')\n",
      "(42, '0.047*\"bought\" + 0.037*\"store\" + 0.030*\"box\" + 0.026*\"one\"')\n",
      "(43, '0.071*\"sauce\" + 0.026*\"chicken\" + 0.025*\"hot\" + 0.020*\"bottles\"')\n",
      "(44, '0.044*\"amazon\" + 0.034*\"store\" + 0.033*\"price\" + 0.023*\"much\"')\n",
      "(45, '0.050*\"honey\" + 0.026*\"best\" + 0.024*\"right\" + 0.024*\"one\"')\n",
      "(46, '0.050*\"free\" + 0.040*\"gluten\" + 0.030*\"pasta\" + 0.029*\"products\"')\n",
      "(47, '0.028*\"back\" + 0.026*\"bring\" + 0.024*\"home\" + 0.022*\"found\"')\n",
      "(48, '0.075*\"butter\" + 0.053*\"peanut\" + 0.026*\"cocoa\" + 0.025*\"hot\"')\n",
      "(49, '0.020*\"kids\" + 0.018*\"pretzels\" + 0.018*\"since\" + 0.017*\"yummy\"')\n"
     ]
    }
   ],
   "source": [
    "num_topics = 50  # Adjust based on your dataset and needs\n",
    "ldamodel = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "\n",
    "# Print the topics\n",
    "#topics = ldamodel.print_topics(num_words=4)\n",
    "#for topic in topics:\n",
    "#    print(topic)\n",
    "# Print all 50 topics with the top 4 words in each\n",
    "all_topics = ldamodel.print_topics(num_topics=50, num_words=4)\n",
    "for topic in all_topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb3f66-17b7-4a95-a52c-d72b886461e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948bff2-d3ae-4769-9098-6b58813571db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0b714b-016e-44cc-aae9-7eb4567301ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de4829-f463-452b-b58d-cece80eb0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this experiment was to make a column in the df that showed the top 20 distances of the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b714b-29ae-4240-b32f-f81b314eeb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# way slow This passes one vector at a time.  experiments above change this to multiple vectors for a significant savings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def find_top_similarities_and_distances(index, vectors, k=21):\n",
    "    \"\"\"\n",
    "    Find the top k-1 most similar items and their distances for each vector in the given index.\n",
    "    Exclude the first result to skip the query item itself.\n",
    "    \"\"\"\n",
    "    similar_indices = []\n",
    "    distances_list = []\n",
    "    \n",
    "    # Perform the search for each vector\n",
    "    for i in tqdm(range(vectors.shape[0]), desc='Finding Similarities and Distances'):\n",
    "        D, I = index.search(vectors[i:i+1], k)  # Search for the top k similar items\n",
    "        similar_indices.append(I[0][1:])  # Skip the first result and keep the next 20\n",
    "        distances_list.append(D[0][1:])  # Skip the distance to itself and keep the next 20\n",
    "    \n",
    "    return similar_indices, distances_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb137bdc-31d3-4f94-b99f-46485636eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# way slow 128 days each\n",
    "# Example for the summary vectors and index\n",
    "top20_summary_indices, top20_summary_distances = find_top_similarities_and_distances(index_summary, summary_vectors, k=21)\n",
    "\n",
    "# Update the DataFrame\n",
    "df_sentences['Top20_Summary_Indices'] = top20_summary_indices\n",
    "df_sentences['Top20_Summary_Distances'] = top20_summary_distances\n",
    "\n",
    "# Repeat for sentence and paragraph vectors and indexes\n",
    "top20_sentence_indices, top20_sentence_distances = find_top_similarities_and_distances(index_sentence, sentence_vectors, k=21)\n",
    "df_sentences['Top20_Sentence_Indices'] = top20_sentence_indices\n",
    "df_sentences['Top20_Sentence_Distances'] = top20_sentence_distances\n",
    "\n",
    "top20_paragraph_indices, top20_paragraph_distances = find_top_similarities_and_distances(index_paragraph, paragraph_vectors, k=21)\n",
    "df_paragraph['Top20_Paragraph_Indices'] = top20_paragraph_indices\n",
    "df_paragraph['Top20_Paragraph_Distances'] = top20_paragraph_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1849ce-e312-44be-92a7-560130a44363",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop # some of the code below duplicates the code in the above block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a90212-4a20-4318-8aff-54ec2f32e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pickle_sentences_filename_clusters = os.path.join(download_dir,\"amazon_reviews_pickle_sentences_20240129_clusters.pkl\")\n",
    "\n",
    "df_sentences.to_pickle(df_pickle_sentences_filename_clusters)  # Python pickle format\n",
    "\n",
    "df_pickle_paragraph_filename_index = os.path.join(download_dir,\"amazon_reviews_pickle_paragraph_20240129_index.pkl\")\n",
    "\n",
    "faiss.write_index(index_paragraph, \"index_paragraph.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead0587-0c57-419f-9bcb-0ffcac9627e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pickle_sentences_filename_index = os.path.join(download_dir,\"amazon_reviews_pickle_sentences_20240129_index.pkl\")\n",
    "faiss.write_index(index_sentence, \"index_paragraph.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711865a6-0660-436c-9a6c-297ee9eb3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pickle_paragraph_filename_clusters = os.path.join(download_dir,\"amazon_reviews_pickle_paragraph_20240129_clusters.pkl\")\n",
    "\n",
    "df_paragraph.to_pickle(df_pickle_paragraph_filename_clusters)  # Python pickle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2b5ef5-d5a8-45ad-80f6-9c66ec22d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pickle_paragraph_filename_index = os.path.join(download_dir,\"amazon_reviews_pickle_paragraph_20240129_index.pkl\")\n",
    "\n",
    "faiss.write_index(index_paragraph, \"index_paragraph.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04e6211-da9a-4eb9-8673-d4c7af9cfd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_sentences.iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89041738-bdc7-457c-a85a-995ea84c3fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f744f3b-4f9e-4b30-843b-31e0ff91ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]['Summary_vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21388b9-0491-4810-ab5a-4d4aa53bb063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Retrieve vectors from your DataFrame\n",
    "vector1 = df.iloc[0]['Processed_vector']\n",
    "vector2 = df.iloc[1]['Processed_vector']\n",
    "\n",
    "# Ensure both vectors are numpy arrays and have the same shape\n",
    "vector1 = np.array(vector1, dtype=np.float32).reshape(1, -1)\n",
    "vector2 = np.array(vector2, dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "# Verify shapes are correct (this step is just for demonstration and can be removed later)\n",
    "print(f\"Vector1 Shape: {vector1.shape}\")\n",
    "print(f\"Vector2 Shape: {vector2.shape}\")\n",
    "\n",
    "# Combine vectors into a single 2D array\n",
    "vectors = np.vstack((vector1, vector2))\n",
    "\n",
    "# Continue with FAISS index creation and querying\n",
    "dimension = vectors.shape[1]  # Dimension of the vectors\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance for similarity measure\n",
    "index.add(vectors)  # Add vectors to the index\n",
    "\n",
    "# Perform a query with the first vector\n",
    "query_vector = np.array([vector1], dtype=np.float32).reshape(1, -1)\n",
    "k = 2  # Find the 2 nearest neighbors, including the query vector itself\n",
    "distances, indices = index.search(query_vector, k)\n",
    "\n",
    "# Display the results\n",
    "print(\"Distances:\", distances)\n",
    "print(\"Indices:\", indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55350ae4-33e2-44f6-b691-2862d62d1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Prepare the vectors\n",
    "# Assuming vector1 is the one you provided and vector2 is similarly obtained\n",
    "vector1 = [1.07387170e-01,  1.54322982e-02, ...]  # Truncated for brevity\n",
    "vector2 = [ ... ]  # Similar structure to vector1\n",
    "vector1 = df.iloc[0]['Summary_vector']\n",
    "vector2 = df.iloc[1]['Summary_vector']\n",
    "\n",
    "vectors = np.array([vector1, vector2], dtype=np.float32)\n",
    "\n",
    "# Step 2: Create a FAISS index\n",
    "dimension = vectors.shape[1]  # Dimension of the vectors\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance for similarity measure\n",
    "\n",
    "# Step 3: Add the vectors to the index\n",
    "index.add(vectors)\n",
    "\n",
    "# Step 4: Perform a query\n",
    "# Use the first vector as a query to find its nearest neighbor\n",
    "# Note: Including itself, so the nearest neighbor will be itself for k=1\n",
    "query_vector = np.array([vector1], dtype=np.float32)\n",
    "k = 2  # Find the 2 nearest neighbors, including the query vector itself\n",
    "distances, indices = index.search(query_vector, k)\n",
    "\n",
    "# Display the results\n",
    "print(\"Distances:\", distances)\n",
    "print(\"Indices:\", indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e921ae5-3d4e-4249-91e0-2dfc2b1f4f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033425f-cdfb-4f58-8d41-af57e1010d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiment = df_sentences.head(1000).copy()  # Assuming 'df' is your original DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3b425-a454-432c-848f-50a853b86d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_vectors_for_faiss(vector):\n",
    "    # Ensure the vector is a numpy array of type float32 and is 2D\n",
    "    if isinstance(vector, list):  # If the vector is stored as a list\n",
    "        vector = np.array(vector, dtype=np.float32)\n",
    "    if vector.ndim == 1:\n",
    "        vector = vector.reshape(1, -1)\n",
    "    return vector\n",
    "\n",
    "# Apply this function to the 'Summary_vector' column\n",
    "df_experiment['Summary_vector'] = df_experiment['Summary_vector'].apply(preprocess_vectors_for_faiss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76c4864-2f59-4a35-beb2-30473d5f39ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Assuming you have preprocessed the vectors as 2D arrays\n",
    "# Flatten the arrays for FAISS index, as it expects flat arrays\n",
    "vectors = np.vstack(df_experiment['Summary_vector'].to_numpy())\n",
    "\n",
    "# Create a FAISS index\n",
    "dimension = vectors.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(vectors.astype(np.float32))  # Ensure data is float32\n",
    "\n",
    "# Query the index (example)\n",
    "query_vector = vectors[0:1]  # Use the first vector as a query example\n",
    "k = 5  # Number of nearest neighbors to find\n",
    "distances, indices = index.search(query_vector, k)\n",
    "\n",
    "print(distances, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541afe1-878e-4402-bd21-ba91fcd41377",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf6c6d-dfb2-4727-98fa-ce35b1cf2435",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea5dd36-07f8-45c3-9d93-1674e9ba09e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vectors = index.ntotal\n",
    "vector_dim = index.d\n",
    "print(f\"Number of vectors: {n_vectors}, Dimensionality: {vector_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a062da5c-5510-4049-8471-9397f7e4f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_trained = index.is_trained\n",
    "print(f\"Is the index trained? {is_trained}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355be544-b3c2-4204-8f93-b1aed537f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(index, faiss.IndexIVFFlat):\n",
    "    print(f\"Number of list: {index.nlist}\")\n",
    "    print(f\"Number of probes: {index.nprobe}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e6602-3f72-4416-b36c-4095901ff3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the index to disk\n",
    "index_file = \"my_index.faiss\"\n",
    "faiss.write_index(index, index_file)\n",
    "\n",
    "# Load the index (for inspection or use in another program)\n",
    "index_loaded = faiss.read_index(index_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6ef2e-032d-4a6a-a76b-025b771c7c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MinIO client and BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def generate_vectors(text):\n",
    "    # Check if GPU is available and use it; otherwise, use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Send model to device (GPU or CPU)\n",
    "    model.to(device)\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "    # Ensure no gradient calculations\n",
    "    with torch.no_grad():\n",
    "        # Prepare inputs and send them to the device\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        # Forward pass, send model outputs back to CPU\n",
    "        outputs = model(**inputs).last_hidden_state.mean(dim=1).squeeze().to('cpu').numpy()\n",
    "\n",
    "    # Convert the output to float32 for compatibility and ensure it's flat\n",
    "    return outputs.astype(np.float32)\n",
    "\n",
    "# Example usage with a DataFrame\n",
    "# df_sentence['Summary_vector'] = df_sentence['Summary'].progress_apply(lambda x: generate_vectors(x) if isinstance(x, str) else np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2065c5a-ec84-4aa9-a251-01c540b0a023",
   "metadata": {},
   "source": [
    "# junk end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbbe912-f33b-4617-97cd-4c27c3d50893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3942d041-205a-4c8b-94b8-0afd21e86eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.stack(df_sentences['Summary_vector'].values).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97864dba-287b-419d-be47-d54a8cc0c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = df_sentences['Summary_vector'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2777360-c6ad-4ddf-896b-5786e5dad98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16edf25e-24b4-44c7-b601-2b278dd0b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = df_sentences['Summary_vector'].values.astype(np.float32)  # Assuming vectors are stored as numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1902d14a-dd14-4685-bdfb-4d6fc4b56a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ff635-1acd-4252-9496-5ce53e20392d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973b2975-027b-43da-a400-bed66fbdc7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582bb508-2fd5-40c7-adaa-94ba012c7492",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838fad9-aa7a-49d2-b6de-f961ab17791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Initialize the Chroma client with a specified database path\n",
    "db_path = download_dir  # Specify the path where you want to store the ChromaDB\n",
    "#chroma_client = chromadb.Client(path=db_path) # this gave an error\n",
    "#client = chromadb.PersistentClient(path=\"/path/to/save/to\") # from web site\n",
    "chroma_client = chromadb.PersistentClient(path=db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb899ea-ee4c-48a0-8d73-565338ef377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_metadata_and_vectors(df):\n",
    "    vector_columns = [col for col in df.columns if 'vector' in col.lower()]\n",
    "    metadata_columns = [col for col in df.columns if 'vector' not in col.lower()]\n",
    "    return metadata_columns, vector_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65784f74-d1ff-4147-99cb-54667e2e3216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b38e7a7-bb5d-46bc-8410-2dc87080af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_to_chromadb(df, collection_name, text_columns, embedding_columns, db_path, ignore_columns=None):\n",
    "    if ignore_columns is None:\n",
    "        ignore_columns = []\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=db_path)\n",
    "\n",
    "    try:\n",
    "        collection = chroma_client.get_collection(collection_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Collection {collection_name} not found, creating a new one. Error: {e}\")\n",
    "        collection = chroma_client.create_collection(name=collection_name)\n",
    "\n",
    "    #for _, row in df.iterrows():\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Loading to {collection_name}\"):\n",
    "        metadata = {col: row[col] for col in row.index if col not in embedding_columns + text_columns + ignore_columns}\n",
    "        #embeddings = [row[col] for col in embedding_columns if col in row and col not in ignore_columns]\n",
    "\n",
    "        # Ensure embeddings are converted to list if they are numpy arrays\n",
    "        embeddings = [row[col].tolist() for col in embedding_columns if col in row and col not in ignore_columns and isinstance(row[col], np.ndarray)][0]\n",
    "        embeddings = embeddings[0] if embeddings else None\n",
    "        if embeddings is not None:\n",
    "            embeddings_list.append(embeddings)\n",
    "        \n",
    "        documents = [row[col] for col in text_columns if col in row]\n",
    "        \n",
    "        document_id = f\"{collection_name}_{row['Id']}\"\n",
    "        if 'P_index' in row and 'S_sentence_number' in row:\n",
    "            document_id += f\"_{row['P_index']}_{row['S_sentence_number']}\"\n",
    "\n",
    "        collection.add(\n",
    "            documents=documents,\n",
    "            embeddings=embeddings,  # Each embedding is added separately\n",
    "            metadatas=[metadata],\n",
    "            ids=[document_id]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754b20d-dfe7-4b77-88a4-327d09fa297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_collection(collection):\n",
    "    try:\n",
    "        chroma_client.delete_collection(collection)\n",
    "        print(\"Collection '{collection}' deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete collection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0f884-938d-4e1c-bb88-e0cd47bd68ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_collection('sentences_collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aa9710-cb6e-4c0c-9e35-8d977151fc14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e30de76-d0e9-42fc-886b-2c6090d45b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collections\n",
    "\n",
    "delete_collection('sentences_collection')\n",
    "sentences_collection = chroma_client.create_collection(name=\"sentences_collection\")\n",
    "\n",
    "\n",
    "delete_collection('paragraphs_collection')\n",
    "paragraphs_collection = chroma_client.create_collection(name=\"paragraphs_collection\")\n",
    "\n",
    "\n",
    "delete_collection('paragraphs_collection')\n",
    "summary_collection = chroma_client.create_collection(name=\"paragraphs_collection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9c647-b793-4f2f-bc66-12f88c382407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(df_sentences.iloc[0]['Sentence_vector'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4950d653-f3e9-4b72-81e9-2d1a7cd116e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sentences.iloc[0]['Sentence_vector'][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f4d98-c455-4963-89fc-15cd42ff3349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
