{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fecea18-9008-43ff-bac9-60f85a4ff3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\johnny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "from io import BytesIO\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "# Ensure that the Punkt Tokenizer Models are downloaded\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cad7802-d999-4ed0-89ce-838d81f86ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\downloads\\amazon_customer_reviews\n"
     ]
    }
   ],
   "source": [
    "#download_dir = 'D:\\\\downloads'\n",
    "download_dir = 'D:\\\\downloads\\\\amazon_customer_reviews'\n",
    "print(download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "954b0935-ab78-4fbb-afd7-91dd2c1dbc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reviews.csv']\n"
     ]
    }
   ],
   "source": [
    "download_files = os.listdir(download_dir)\n",
    "print(download_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e71788b-6057-4080-9ecc-c2388ad9e860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          filepaths extensions    filenames\n",
      "0  D:\\downloads\\amazon_customer_reviews\\Reviews.csv       .csv  Reviews.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Populate the list with file information\n",
    "for file in download_files:\n",
    "    filepath = os.path.join(download_dir, file)\n",
    "    filename, file_extension = os.path.splitext(file)\n",
    "    data.append({'filepaths': filepath, 'extensions': file_extension, 'filenames': file})\n",
    "\n",
    "# Create DataFrame from the list\n",
    "files_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows\n",
    "print(files_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "862a6f90-f110-496d-9482-4bb97e562ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\downloads\\\\amazon_customer_reviews\\\\Reviews.csv']\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame for PDF files\n",
    "csv_files_df = files_df[files_df['extensions'] == '.csv']\n",
    "\n",
    "# Create a list of file paths for PDF files\n",
    "csv_filepaths = csv_files_df['filepaths'].tolist()\n",
    "\n",
    "# Display the list\n",
    "print(csv_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc429c77-3085-447d-9a70-5601f6bfa4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\downloads\\amazon_customer_reviews\\Reviews.csv\n"
     ]
    }
   ],
   "source": [
    "reviews_file_path = csv_filepaths[0]\n",
    "print(reviews_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61fbb988-74e4-4b83-a638-6b6ed46b06c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     1                       1      5  1303862400   \n",
      "1                     0                       0      1  1346976000   \n",
      "2                     1                       1      4  1219017600   \n",
      "3                     3                       3      2  1307923200   \n",
      "4                     0                       0      5  1350777600   \n",
      "\n",
      "                 Summary                                          Paragraph  \n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...  \n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df_paragraph = pd.read_csv(reviews_file_path)\n",
    "df_paragraph.rename(columns={'Text': 'Paragraph'}, inplace=True)\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df_paragraph.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8ae60e7-90e5-4f0d-8756-16c9f6ecdf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_paragraphs_into_sentences(df):\n",
    "    # Create a list to store the new rows\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate through each row in the input DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Use nltk to split the paragraph into sentences\n",
    "        sentences = nltk.tokenize.sent_tokenize(row['Paragraph'])\n",
    "\n",
    "        # Add each sentence as a new row, keeping other columns the same\n",
    "        for sentence_number, sentence in enumerate(sentences, start=1):\n",
    "            new_row = row.to_dict()\n",
    "            new_row['P_index'] = f'P_{index}'\n",
    "            new_row['S_sentence_number'] = f'S_{sentence_number}'\n",
    "            new_row['Sentence'] = sentence\n",
    "            # Ensure 'Paragraph' column is not duplicated\n",
    "            del new_row['Paragraph']\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "    # Create a new DataFrame with the new rows\n",
    "    df_sentence = pd.DataFrame(new_rows)\n",
    "\n",
    "    return df_sentence\n",
    "\n",
    "# Example usage:\n",
    "# df_paragraph = pd.DataFrame({'Paragraph': [\"Your paragraphs here.\"], ...other columns...})\n",
    "# df_sentence = split_paragraphs_into_sentences(df_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9849fec5-3f65-4d3e-81ce-a08e2abeb352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_paragraphs_into_sentences_1(df):\n",
    "    # Create a list to store the new rows\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate through each row in the input DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Use nltk to split the paragraph into sentences\n",
    "        sentences = nltk.tokenize.sent_tokenize(row['Text'])\n",
    "\n",
    "        # Add each sentence as a new row, keeping other columns the same\n",
    "        for sentence_number, sentence in enumerate(sentences, start=1):\n",
    "            new_row = row.to_dict()            \n",
    "            new_row['P_index'] = f'P_{index}'\n",
    "            new_row['S_sentence_number'] = f'S_{sentence_number}'\n",
    "            new_row['Sentence'] = sentence\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "    # Create a new DataFrame with the new rows\n",
    "    df_sentence = pd.DataFrame(new_rows)\n",
    "\n",
    "    return df_sentence\n",
    "\n",
    "# Example usage:\n",
    "# df_paragraph = pd.DataFrame({'Text': [\"Your paragraphs here.\"], ...other columns...})\n",
    "# df_sentence = split_paragraphs_into_sentences(df_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b1338a-006a-4d72-aa91-4cb6842b28cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55aeadc3-ac24-458e-977a-bdccfd7fea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence = split_paragraphs_into_sentences(df_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad9d7b67-8d46-4a6f-b163-77785ab63fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'P_index',\n",
       "       'S_sentence_number', 'Sentence'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentence.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b800dc75-b5c8-477c-b620-170176f5a466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have bought several of the Vitality canned dog food products and have found them all to be of good quality.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentence.iloc[0]['Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28150841-7937-40d0-b1bc-b0237c97f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MinIO client and BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8370f715-71ba-4217-98a6-4e5a5554b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vectors(text):\n",
    "    # Check if GPU is available and use it; otherwise, use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Send model to device (GPU or CPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Ensure no gradient calculations\n",
    "    with torch.no_grad():\n",
    "        # Prepare inputs and send them to the device\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        # Forward pass, send model outputs back to CPU\n",
    "        outputs = model(**inputs).last_hidden_state.mean(dim=1).to('cpu')\n",
    "\n",
    "    return outputs.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f8c62459-f17b-4a09-a2a8-f745ea29920a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947134a6f48841b3bc008abb9ea37d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/2832806 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# progress bar\n",
    "#from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm_notebook.pandas(desc=\"processing\")\n",
    "\n",
    "\n",
    "#df_sentence['Summary_vector'] = df_sentence['Summary'].apply(lambda x: generate_vectors(x) if isinstance(x, str) else np.nan)\n",
    "df_sentence['Summary_vector'] = df_sentence['Summary'].progress_apply(lambda x: generate_vectors(x) if isinstance(x, str) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b3dc392b-5cad-4de0-b54a-d6cb07442703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c8aa71d2a8448182faf63befcbdfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/2832806 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sentence['Sentence_vector'] = df_sentence['Sentence'].progress_apply(lambda x: generate_vectors(x) if isinstance(x, str) else np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9fca64bd-9bc6-478f-9688-b6ddc02dd692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pickle_filename = os.path.join(download_dir,\"amazon_reviews_pickle_sentences\")\n",
    "df_sentence.to_pickle(df_pickle_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "280d88fe-6edb-4c96-bed6-7fbad316f3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>P_index</th>\n",
       "      <th>S_sentence_number</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Summary_vector</th>\n",
       "      <th>Sentence_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>453152</th>\n",
       "      <td>90507</td>\n",
       "      <td>B000EFFTR0</td>\n",
       "      <td>AZNB1YMYG0JPT</td>\n",
       "      <td>John E. Tipps</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1321574400</td>\n",
       "      <td>Hamburger Helper Potato Stroganoff</td>\n",
       "      <td>P_90506</td>\n",
       "      <td>S_7</td>\n",
       "      <td>!</td>\n",
       "      <td>[[0.031706095, -0.29236707, 0.08684268, 0.0522...</td>\n",
       "      <td>[[0.24765381, -0.117233835, -0.2429299, -0.216...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789314</th>\n",
       "      <td>157004</td>\n",
       "      <td>B001GINOQC</td>\n",
       "      <td>A3XG8M5E5ALNP</td>\n",
       "      <td>CG in NC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1284940800</td>\n",
       "      <td>Great choice for a snack</td>\n",
       "      <td>P_157003</td>\n",
       "      <td>S_2</td>\n",
       "      <td>Everyone I have had try them, loves them!</td>\n",
       "      <td>[[0.0770887, -0.1289951, 0.25035286, 0.1587796...</td>\n",
       "      <td>[[0.5010618, 0.3455162, 0.17572194, -0.2641657...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221269</th>\n",
       "      <td>44662</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A3QVAKVRAH657N</td>\n",
       "      <td>Orrin C. Judd \"brothersjudddotcom\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1211587200</td>\n",
       "      <td>EVEN THE WIFE WAS HAPPY</td>\n",
       "      <td>P_44661</td>\n",
       "      <td>S_7</td>\n",
       "      <td>These latter are most notable because they man...</td>\n",
       "      <td>[[-0.10866005, -0.19496475, 0.19317056, 0.1024...</td>\n",
       "      <td>[[-0.17071877, 0.21959276, 0.35957518, 0.11829...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068171</th>\n",
       "      <td>413742</td>\n",
       "      <td>B0026KPDG8</td>\n",
       "      <td>A3TQ0ZSAD2G98A</td>\n",
       "      <td>Madeline C. Kimmich</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1319846400</td>\n",
       "      <td>Salt and Pepper!!!</td>\n",
       "      <td>P_413741</td>\n",
       "      <td>S_2</td>\n",
       "      <td>We bought a variety pack.</td>\n",
       "      <td>[[0.06598242, 0.23023576, 0.09852357, -0.48538...</td>\n",
       "      <td>[[0.71364367, -0.44887614, -0.012389574, 0.369...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500777</th>\n",
       "      <td>99744</td>\n",
       "      <td>B003I6OEMI</td>\n",
       "      <td>A1AOS6DFYSOGC4</td>\n",
       "      <td>Jcallie</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1282521600</td>\n",
       "      <td>Awesome Thomas Pez Dispenser</td>\n",
       "      <td>P_99743</td>\n",
       "      <td>S_1</td>\n",
       "      <td>Awesome Thomas pez dispensers.</td>\n",
       "      <td>[[0.061932158, -0.10549656, 0.1725223, 0.29008...</td>\n",
       "      <td>[[0.08059786, -0.054100454, 0.014445701, 0.310...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id   ProductId          UserId  \\\n",
       "453152    90507  B000EFFTR0   AZNB1YMYG0JPT   \n",
       "789314   157004  B001GINOQC   A3XG8M5E5ALNP   \n",
       "221269    44662  B001EQ55RW  A3QVAKVRAH657N   \n",
       "2068171  413742  B0026KPDG8  A3TQ0ZSAD2G98A   \n",
       "500777    99744  B003I6OEMI  A1AOS6DFYSOGC4   \n",
       "\n",
       "                                ProfileName  HelpfulnessNumerator  \\\n",
       "453152                        John E. Tipps                     0   \n",
       "789314                             CG in NC                     0   \n",
       "221269   Orrin C. Judd \"brothersjudddotcom\"                     0   \n",
       "2068171                 Madeline C. Kimmich                     0   \n",
       "500777                              Jcallie                     0   \n",
       "\n",
       "         HelpfulnessDenominator  Score        Time  \\\n",
       "453152                        0      5  1321574400   \n",
       "789314                        0      5  1284940800   \n",
       "221269                        0      5  1211587200   \n",
       "2068171                       0      5  1319846400   \n",
       "500777                        0      5  1282521600   \n",
       "\n",
       "                                    Summary   P_index S_sentence_number  \\\n",
       "453152   Hamburger Helper Potato Stroganoff   P_90506               S_7   \n",
       "789314             Great choice for a snack  P_157003               S_2   \n",
       "221269              EVEN THE WIFE WAS HAPPY   P_44661               S_7   \n",
       "2068171                  Salt and Pepper!!!  P_413741               S_2   \n",
       "500777         Awesome Thomas Pez Dispenser   P_99743               S_1   \n",
       "\n",
       "                                                  Sentence  \\\n",
       "453152                                                   !   \n",
       "789314           Everyone I have had try them, loves them!   \n",
       "221269   These latter are most notable because they man...   \n",
       "2068171                          We bought a variety pack.   \n",
       "500777                      Awesome Thomas pez dispensers.   \n",
       "\n",
       "                                            Summary_vector  \\\n",
       "453152   [[0.031706095, -0.29236707, 0.08684268, 0.0522...   \n",
       "789314   [[0.0770887, -0.1289951, 0.25035286, 0.1587796...   \n",
       "221269   [[-0.10866005, -0.19496475, 0.19317056, 0.1024...   \n",
       "2068171  [[0.06598242, 0.23023576, 0.09852357, -0.48538...   \n",
       "500777   [[0.061932158, -0.10549656, 0.1725223, 0.29008...   \n",
       "\n",
       "                                           Sentence_vector  \n",
       "453152   [[0.24765381, -0.117233835, -0.2429299, -0.216...  \n",
       "789314   [[0.5010618, 0.3455162, 0.17572194, -0.2641657...  \n",
       "221269   [[-0.17071877, 0.21959276, 0.35957518, 0.11829...  \n",
       "2068171  [[0.71364367, -0.44887614, -0.012389574, 0.369...  \n",
       "500777   [[0.08059786, -0.054100454, 0.014445701, 0.310...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentence.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e4cb71c3-91c8-4609-baff-2b33f942f4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame size: 2356.73 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "size_in_bytes = sys.getsizeof(df_sentence)\n",
    "size_in_mb = size_in_bytes / (1024**2)  # Convert to Megabytes\n",
    "print(\"DataFrame size: {:.2f} MB\".format(size_in_mb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c58ef8ac-a34f-4f30-8b79-fcb1a8464c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "665"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c04a4332-d5fc-4626-8c18-7c372656cf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_sentence: 2471212020 bytes\n",
      "_78: 2471212020 bytes\n",
      "_20: 1700325293 bytes\n",
      "df_paragraph: 467442561 bytes\n",
      "csv_df: 467424279 bytes\n",
      "_86: 4216 bytes\n",
      "BertTokenizer: 2008 bytes\n",
      "BertModel: 2008 bytes\n",
      "tqdm: 2008 bytes\n",
      "tqdm_notebook: 2008 bytes\n",
      "_28: 1883 bytes\n",
      "all_objects: 1656 bytes\n",
      "sorted_objects: 1528 bytes\n",
      "___: 1464 bytes\n",
      "_89: 1464 bytes\n",
      "_i33: 1119 bytes\n",
      "_35: 1118 bytes\n",
      "_27: 1114 bytes\n",
      "_i16: 1090 bytes\n",
      "_i63: 1090 bytes\n",
      "AutoModel: 1064 bytes\n",
      "AutoTokenizer: 1064 bytes\n",
      "_i31: 1028 bytes\n",
      "_i62: 1015 bytes\n",
      "_80: 981 bytes\n",
      "_i54: 947 bytes\n",
      "_i57: 947 bytes\n",
      "_i58: 942 bytes\n",
      "_i52: 941 bytes\n",
      "_i59: 941 bytes\n",
      "_i42: 940 bytes\n",
      "_i46: 934 bytes\n",
      "_i44: 933 bytes\n",
      "_ih: 920 bytes\n",
      "In: 920 bytes\n",
      "_i40: 822 bytes\n",
      "_i25: 675 bytes\n",
      "_i66: 675 bytes\n",
      "_oh: 640 bytes\n",
      "Out: 640 bytes\n",
      "_i11: 484 bytes\n",
      "csv_files_df: 476 bytes\n",
      "files_df: 476 bytes\n",
      "_i3: 472 bytes\n",
      "_i7: 472 bytes\n",
      "_i75: 445 bytes\n",
      "_i83: 436 bytes\n",
      "_iii: 422 bytes\n",
      "_i84: 422 bytes\n",
      "_i88: 422 bytes\n",
      "_i92: 422 bytes\n",
      "_i95: 422 bytes\n",
      "_i41: 420 bytes\n",
      "_i70: 411 bytes\n",
      "_i71: 409 bytes\n",
      "datetime: 408 bytes\n",
      "BytesIO: 408 bytes\n",
      "_i72: 405 bytes\n",
      "_i74: 396 bytes\n",
      "_i37: 390 bytes\n",
      "_i73: 387 bytes\n",
      "df: 378 bytes\n",
      "_50: 323 bytes\n",
      "_i23: 305 bytes\n",
      "_i47: 305 bytes\n",
      "_i53: 305 bytes\n",
      "_i56: 305 bytes\n",
      "_i43: 299 bytes\n",
      "_i45: 299 bytes\n",
      "_i55: 299 bytes\n",
      "_i65: 299 bytes\n",
      "_i64: 298 bytes\n",
      "_i12: 280 bytes\n",
      "_i32: 269 bytes\n",
      "_i8: 268 bytes\n",
      "_i39: 260 bytes\n",
      "_i21: 254 bytes\n",
      "_i81: 253 bytes\n",
      "_i82: 252 bytes\n",
      "df_pickle_filename: 225 bytes\n",
      "_i17: 218 bytes\n",
      "_i51: 216 bytes\n",
      "_i22: 207 bytes\n",
      "_i24: 207 bytes\n",
      "_i18: 205 bytes\n",
      "filepath: 195 bytes\n",
      "reviews_file_path: 195 bytes\n",
      "_i14: 193 bytes\n",
      "_i69: 187 bytes\n",
      "_i76: 185 bytes\n",
      "_i67: 180 bytes\n",
      "_i68: 174 bytes\n",
      "_i85: 168 bytes\n",
      "_i87: 166 bytes\n",
      "_i77: 162 bytes\n",
      "download_dir: 159 bytes\n",
      "_30: 158 bytes\n",
      "_36: 158 bytes\n",
      "_i5: 156 bytes\n",
      "_i15: 139 bytes\n",
      "open: 136 bytes\n",
      "split_paragraphs_into_sentences: 136 bytes\n",
      "generate_vectors: 136 bytes\n",
      "split_paragraphs_into_sentences_1: 136 bytes\n",
      "process_text_to_vectors: 136 bytes\n",
      "trange: 136 bytes\n",
      "_i4: 127 bytes\n",
      "__doc__: 113 bytes\n",
      "_i6: 112 bytes\n",
      "_i10: 110 bytes\n",
      "_i13: 110 bytes\n",
      "_i19: 108 bytes\n",
      "_i34: 108 bytes\n",
      "_i50: 107 bytes\n",
      "_i91: 103 bytes\n",
      "model_name: 102 bytes\n",
      "__session__: 89 bytes\n",
      "__: 88 bytes\n",
      "data: 88 bytes\n",
      "download_files: 88 bytes\n",
      "_90: 88 bytes\n",
      "_i9: 85 bytes\n",
      "_i36: 80 bytes\n",
      "_i2: 77 bytes\n",
      "_i29: 76 bytes\n",
      "_i30: 76 bytes\n",
      "_i48: 74 bytes\n",
      "cwd: 73 bytes\n",
      "_i61: 73 bytes\n",
      "__builtin__: 72 bytes\n",
      "__builtins__: 72 bytes\n",
      "os: 72 bytes\n",
      "pd: 72 bytes\n",
      "np: 72 bytes\n",
      "nltk: 72 bytes\n",
      "torch: 72 bytes\n",
      "_i49: 72 bytes\n",
      "sleep: 72 bytes\n",
      "sys: 72 bytes\n",
      "gc: 72 bytes\n",
      "_i: 71 bytes\n",
      "_i79: 71 bytes\n",
      "_i94: 71 bytes\n",
      "_i26: 70 bytes\n",
      "_i86: 70 bytes\n",
      "_i80: 69 bytes\n",
      "_i27: 68 bytes\n",
      "_i28: 68 bytes\n",
      "_i35: 68 bytes\n",
      "_i90: 68 bytes\n",
      "_i60: 66 bytes\n",
      "_dh: 64 bytes\n",
      "get_ipython: 64 bytes\n",
      "csv_filepaths: 64 bytes\n",
      "_i89: 63 bytes\n",
      "_ii: 61 bytes\n",
      "_i93: 61 bytes\n",
      "file: 60 bytes\n",
      "_i20: 60 bytes\n",
      "_i78: 60 bytes\n",
      "_i1: 58 bytes\n",
      "__name__: 57 bytes\n",
      "name: 57 bytes\n",
      "obj: 57 bytes\n",
      "filename: 56 bytes\n",
      "element: 56 bytes\n",
      "file_extension: 53 bytes\n",
      "_i38: 53 bytes\n",
      "exit: 48 bytes\n",
      "quit: 48 bytes\n",
      "tokenizer: 48 bytes\n",
      "model: 48 bytes\n",
      "_60: 48 bytes\n",
      "size_in_bytes: 32 bytes\n",
      "_: 28 bytes\n",
      "_17: 28 bytes\n",
      "_21: 28 bytes\n",
      "_23: 28 bytes\n",
      "_37: 28 bytes\n",
      "_41: 28 bytes\n",
      "i: 28 bytes\n",
      "j: 28 bytes\n",
      "size: 28 bytes\n",
      "_94: 28 bytes\n",
      "size_in_mb: 24 bytes\n",
      "__package__: 16 bytes\n",
      "__loader__: 16 bytes\n",
      "__spec__: 16 bytes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Create a list of tuples from the global symbol table to avoid RuntimeError\n",
    "all_objects = [(name, sys.getsizeof(obj)) for name, obj in globals().items()]\n",
    "\n",
    "# Sort the list by size\n",
    "sorted_objects = sorted(all_objects, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the object names and their sizes\n",
    "for name, size in sorted_objects:\n",
    "    print(f\"{name}: {size} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "77c609f8-9d60-4562-93f9-9845de0e170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('___', 2471212020)\n",
      "('df_sentence', 2471212020)\n",
      "('_78', 2471212020)\n",
      "('_20', 1700325293)\n"
     ]
    }
   ],
   "source": [
    "for element in sorted_objects[0:4]:\n",
    "    print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9ab5326f-3b61-4cc6-b06b-4ea797be0395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
      "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Paragraph'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.indexes.base.Index'>\n"
     ]
    }
   ],
   "source": [
    "print(__)  \n",
    "print(type(__))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a336a8-2695-467f-bd1b-3c57e684145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca1a2f-4a6f-4a40-925c-f54683861b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "44c3f315-0ba2-427c-9339-aeebb14b8ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Paragraph'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paragraph.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "56a82323-b6de-4ff9-bb21-0c0a7d777d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff46fee8ed9348628556f1a44c1cf1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/568454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_paragraph['Paragraph_vector'] = df_paragraph['Paragraph'].progress_apply(lambda x: generate_vectors(x) if isinstance(x, str) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "47771aa8-cbc7-4920-84b3-1c51121b81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pickle_filename = os.path.join(download_dir,\"amazon_reviews_pickle_paragraphs\")\n",
    "df_paragraph.to_pickle(df_pickle_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa248b42-c106-495e-b9ea-fa9ba47faf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xlsx_filename = os.path.join(download_dir,\"amazon_reviews_pickle_paragraphs.xlsx\")\n",
    "df_paragraph.to_excel(df_xlsx_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "743665be-0c95-46be-b4b1-2497c2d2fee1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91282130-2ef5-4022-a48f-da69183a1396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from chromadb import Chroma\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Id': [1, 2, 3],\n",
    "    'ProductId': ['B0001PB9FE', 'B0001PB9FE', 'B0001PB9FE'],\n",
    "    'UserId': ['A3SGXH7AUHU8GW', 'A1D87F6ZCVE5NK', 'ABXLMWJIXXAIN'],\n",
    "    'ProfileName': ['delmartian', 'dll pa', 'Natalia Corres \"Naty Corres\"'],\n",
    "    'HelpfulnessNumerator': [1, 0, 1],\n",
    "    'HelpfulnessDenominator': [1, 0, 1],\n",
    "    'Score': [5, 1, 4],\n",
    "    'Time': [1303862400, 1346976000, 1219017600],\n",
    "    'Summary': ['Good Quality Dog Food', 'Not as Advertised', 'Delightful'],\n",
    "    'P_index': [0, 1, 2],\n",
    "    'S_sentence_number': [0, 1, 2],\n",
    "    'Sentence': ['I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.', 'Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".', 'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.']\n",
    "    'Summary_vector': [[0.1, 0.2, ..., 0.3], [0.4, 0.5, ..., 0.6], [0.7, 0.8, ..., 0.9]],\n",
    "    'Sentence_vector': [[0.1, 0.2, ..., 0.3], [0.4, 0.5, ..., 0.6], [0.7, 0.8, ..., 0.9]]\n",
    "})\n",
    "\n",
    "# Convert the DataFrame into a list of dictionaries\n",
    "dicts = df.to_dict(orient='records')\n",
    "\n",
    "# Convert the list of dictionaries into a list of vectors\n",
    "vectors = [d['Summary_vector'] + d['Sentence_vector'] for d in dicts]\n",
    "\n",
    "# Store the list of vectors in ChromaDB\n",
    "chroma = Chroma()\n",
    "chroma.put(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e9635c-6e5f-4ae7-8d0b-1836cd0c43ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a04ad46-c66a-493e-9e32-5c741bf7b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from chromadb import Chroma\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Id': [1, 2, 3],\n",
    "    'ProductId': ['B0001PB9FE', 'B0001PB9FE', 'B0001PB9FE'],\n",
    "    'UserId': ['A3SGXH7AUHU8GW', 'A1D87F6ZCVE5NK', 'ABXLMWJIXXAIN'],\n",
    "    'ProfileName': ['delmartian', 'dll pa', 'Natalia Corres \"Naty Corres\"'],\n",
    "    'HelpfulnessNumerator': [1, 0, 1],\n",
    "    'HelpfulnessDenominator': [1, 0, 1],\n",
    "    'Score': [5, 1, 4],\n",
    "    'Time': [1303862400, 1346976000, 1219017600],\n",
    "    'Summary': ['Good Quality Dog Food', 'Not as Advertised', 'Delightful'],\n",
    "    'P_index': [0, 1, 2],\n",
    "    'S_sentence_number': [0, 1, 2],\n",
    "    'Sentence': ['I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.', 'Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".', 'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.']\n",
    "    'Summary_vector': [[0.1, 0.2, ..., 0.3], [0.4, 0.5, ..., 0.6], [0.7, 0.8, ..., 0.9]],\n",
    "    'Sentence_vector': [[0.1, 0.2, ..., 0.3], [0.4, 0.5, ..., 0.6], [0.7, 0.8, ..., 0.9]]\n",
    "})\n",
    "\n",
    "# Convert the DataFrame into a list of dictionaries with metadata\n",
    "dicts = df.to_dict(orient='records')\n",
    "\n",
    "# Convert the list of dictionaries into a list of vectors with metadata\n",
    "vectors = [{'vector': d['Summary_vector'] + d['Sentence_vector'], 'metadata': d} for d in dicts]\n",
    "\n",
    "# Store the list of vectors in ChromaDB\n",
    "chroma = Chroma()\n",
    "chroma.put(vectors)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
