{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cda9b1-6dbe-48ff-8c40-c508f49ce98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fecea18-9008-43ff-bac9-60f85a4ff3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\johnny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "from io import BytesIO\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Ensure that the Punkt Tokenizer Models are downloaded\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cad7802-d999-4ed0-89ce-838d81f86ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\downloads\\amazon_customer_reviews\n"
     ]
    }
   ],
   "source": [
    "#download_dir = 'D:\\\\downloads'\n",
    "download_dir = 'D:\\\\downloads\\\\amazon_customer_reviews'\n",
    "print(download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "954b0935-ab78-4fbb-afd7-91dd2c1dbc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4ee3300b-8d78-46ac-8abf-72edb1f4f5db', 'amazon_reviews_pickle_paragraphs.xlsx', 'amazon_reviews_pickle_paragraphs_2240129.pkl', 'amazon_reviews_pickle_sentences', 'amazon_reviews_pickle_sentences_2240129.pkl', 'chroma.sqlite3', 'Reviews.csv']\n"
     ]
    }
   ],
   "source": [
    "download_files = os.listdir(download_dir)\n",
    "print(download_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e71788b-6057-4080-9ecc-c2388ad9e860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           filepaths extensions  \\\n",
      "0  D:\\downloads\\amazon_customer_reviews\\4ee3300b-...              \n",
      "1  D:\\downloads\\amazon_customer_reviews\\amazon_re...      .xlsx   \n",
      "2  D:\\downloads\\amazon_customer_reviews\\amazon_re...       .pkl   \n",
      "3  D:\\downloads\\amazon_customer_reviews\\amazon_re...              \n",
      "4  D:\\downloads\\amazon_customer_reviews\\amazon_re...       .pkl   \n",
      "\n",
      "                                      filenames  \n",
      "0          4ee3300b-8d78-46ac-8abf-72edb1f4f5db  \n",
      "1         amazon_reviews_pickle_paragraphs.xlsx  \n",
      "2  amazon_reviews_pickle_paragraphs_2240129.pkl  \n",
      "3               amazon_reviews_pickle_sentences  \n",
      "4   amazon_reviews_pickle_sentences_2240129.pkl  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Populate the list with file information\n",
    "for file in download_files:\n",
    "    filepath = os.path.join(download_dir, file)\n",
    "    filename, file_extension = os.path.splitext(file)\n",
    "    data.append({'filepaths': filepath, 'extensions': file_extension, 'filenames': file})\n",
    "\n",
    "# Create DataFrame from the list\n",
    "files_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows\n",
    "print(files_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "862a6f90-f110-496d-9482-4bb97e562ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\downloads\\\\amazon_customer_reviews\\\\Reviews.csv']\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame for PDF files\n",
    "csv_files_df = files_df[files_df['extensions'] == '.csv']\n",
    "\n",
    "# Create a list of file paths for PDF files\n",
    "csv_filepaths = csv_files_df['filepaths'].tolist()\n",
    "\n",
    "# Display the list\n",
    "print(csv_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc429c77-3085-447d-9a70-5601f6bfa4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\downloads\\amazon_customer_reviews\\Reviews.csv\n"
     ]
    }
   ],
   "source": [
    "reviews_file_path = csv_filepaths[0]\n",
    "print(reviews_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf387dfd-10cc-41a7-bc9e-71c8d4b82d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df_paragraph = pd.read_csv(reviews_file_path)\n",
    "df_paragraph.rename(columns={'Text': 'Paragraph'}, inplace=True)\n",
    "df_paragraph.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3b64571-6b6b-4d59-97f0-17dd55751496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs in 'Paragraph': 0\n",
      "Number of NaNs in 'Summary': 27\n"
     ]
    }
   ],
   "source": [
    "nan_count_paragraph = df_paragraph['Paragraph'].isna().sum()\n",
    "nan_count_summary = df_paragraph['Summary'].isna().sum()\n",
    "\n",
    "print(f\"Number of NaNs in 'Paragraph': {nan_count_paragraph}\")\n",
    "print(f\"Number of NaNs in 'Summary': {nan_count_summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3b529eb-d3f1-45e8-9756-54ff13c2a2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph     0\n",
      "Summary      27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_counts = df_paragraph[['Paragraph', 'Summary']].isna().sum()\n",
    "\n",
    "print(nan_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61fbb988-74e4-4b83-a638-6b6ed46b06c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning, df_paragraph rows: 568427\n",
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     1                       1      5  1303862400   \n",
      "1                     0                       0      1  1346976000   \n",
      "2                     1                       1      4  1219017600   \n",
      "3                     3                       3      2  1307923200   \n",
      "4                     0                       0      5  1350777600   \n",
      "\n",
      "                 Summary                                          Paragraph  \n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Overwrite df_paragraph with rows where neither 'Paragraph' nor 'Summary' is NaN\n",
    "df_paragraph = df_paragraph.dropna(subset=['Paragraph', 'Summary'])\n",
    "\n",
    "print(f\"After cleaning, df_paragraph rows: {len(df_paragraph)}\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df_paragraph.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "897dc0ad-5a30-4711-9f53-c09243aaf7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568427, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paragraph.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8ae60e7-90e5-4f0d-8756-16c9f6ecdf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_paragraphs_into_sentences_old(df):\n",
    "    # Create a list to store the new rows\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate through each row in the input DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Use nltk to split the paragraph into sentences\n",
    "        sentences = nltk.tokenize.sent_tokenize(row['Paragraph'])\n",
    "\n",
    "        # Add each sentence as a new row, keeping other columns the same\n",
    "        for sentence_number, sentence in enumerate(sentences, start=1):\n",
    "            new_row = row.to_dict()\n",
    "            new_row['P_index'] = f'P_{index}'\n",
    "            new_row['S_sentence_number'] = f'S_{sentence_number}'\n",
    "            new_row['Sentence'] = sentence\n",
    "            # Ensure 'Paragraph' column is not duplicated\n",
    "            del new_row['Paragraph']\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "    # Create a new DataFrame with the new rows\n",
    "    df_sentence = pd.DataFrame(new_rows)\n",
    "\n",
    "    return df_sentence\n",
    "\n",
    "# Example usage:\n",
    "# df_paragraph = pd.DataFrame({'Paragraph': [\"Your paragraphs here.\"], ...other columns...})\n",
    "# df_sentence = split_paragraphs_into_sentences(df_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66b1338a-006a-4d72-aa91-4cb6842b28cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_paragraphs_into_sentences(df):\n",
    "    new_rows = []\n",
    "\n",
    "    # Wrap df.iterrows() with tqdm() for a progress bar\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Splitting paragraphs\"):\n",
    "        sentences = nltk.tokenize.sent_tokenize(row['Paragraph'])\n",
    "\n",
    "        for sentence_number, sentence in enumerate(sentences, start=1):\n",
    "            new_row = row.to_dict()\n",
    "            new_row['P_index'] = f'P_{index}'\n",
    "            new_row['S_sentence_number'] = f'S_{sentence_number}'\n",
    "            new_row['Sentence'] = sentence\n",
    "            del new_row['Paragraph']  # Avoid duplicating 'Paragraph' column\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "    df_sentence = pd.DataFrame(new_rows)\n",
    "    return df_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55aeadc3-ac24-458e-977a-bdccfd7fea13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba12c3d55a0465fa39bf1628d74fb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Splitting paragraphs:   0%|          | 0/568427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sentence = split_paragraphs_into_sentences(df_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad9d7b67-8d46-4a6f-b163-77785ab63fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'P_index',\n",
       "       'S_sentence_number', 'Sentence'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentence.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b800dc75-b5c8-477c-b620-170176f5a466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have bought several of the Vitality canned dog food products and have found them all to be of good quality.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentence.iloc[0]['Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28150841-7937-40d0-b1bc-b0237c97f46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe35f6877c384de5a23ce59d08235f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\johnny\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7201e08e897c44d2a1746dd9474682b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f942458dba54d9788180e3b24eb3c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885c040d836242a1bf0a0c58e674450d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b0268d59e244fcbb334fd36a1ad198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize MinIO client and BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8015b501-3e2a-4d7e-bdaf-71a5d5b66a90",
   "metadata": {},
   "source": [
    "def generate_vectors(text):\n",
    "    # Check if GPU is available and use it; otherwise, use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Send model to device (GPU or CPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Ensure no gradient calculations\n",
    "    with torch.no_grad():\n",
    "        # Prepare inputs and send them to the device\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        # Forward pass, send model outputs back to CPU\n",
    "        outputs = model(**inputs).last_hidden_state.mean(dim=1).to('cpu')\n",
    "\n",
    "    return outputs.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b525c5f-e45b-4c42-ac03-5a1f1f721576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MinIO client and BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def generate_vectors(text):\n",
    "    # Check if GPU is available and use it; otherwise, use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Send model to device (GPU or CPU)\n",
    "    model.to(device)\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "    # Ensure no gradient calculations\n",
    "    with torch.no_grad():\n",
    "        # Prepare inputs and send them to the device\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        \n",
    "        # Forward pass, send model outputs back to CPU\n",
    "        outputs = model(**inputs).last_hidden_state.mean(dim=1).squeeze().to('cpu').numpy()\n",
    "\n",
    "    # Convert the output to float32 for compatibility and ensure it's flat\n",
    "    return outputs.astype(np.float32)\n",
    "\n",
    "# Example usage with a DataFrame\n",
    "# df_sentence['Summary_vector'] = df_sentence['Summary'].progress_apply(lambda x: generate_vectors(x) if isinstance(x, str) else np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62925a37-dc60-4dd0-9da0-e562566e5502",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5ec3c4d-d0ce-4ac9-894e-b3b2eaf7efcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c1d510d16149a4aee0a669d1e8afac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/2832752 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df_sentence['Summary_vector'] = df_sentence['Summary'].apply(lambda x: generate_vectors(x) if isinstance(x, str) else np.nan)\n",
    "df_sentence['Summary_vector'] = df_sentence['Summary'].progress_apply(lambda x: generate_vectors(x) if isinstance(x, str) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3dc392b-5cad-4de0-b54a-d6cb07442703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec5c5994e354e0da457a9f8f6c08207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/2832752 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sentence['Sentence_vector'] = df_sentence['Sentence'].progress_apply(lambda x: generate_vectors(x) if isinstance(x, str) else np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fca64bd-9bc6-478f-9688-b6ddc02dd692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\downloads\\amazon_customer_reviews\\amazon_reviews_pickle_sentences_20240219.pkl\n"
     ]
    }
   ],
   "source": [
    "df_pickle_filename = os.path.join(download_dir,\"amazon_reviews_pickle_sentences_20240219.pkl\")\n",
    "df_sentence.to_pickle(df_pickle_filename)\n",
    "print(df_pickle_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "280d88fe-6edb-4c96-bed6-7fbad316f3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>P_index</th>\n",
       "      <th>S_sentence_number</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Summary_vector</th>\n",
       "      <th>Sentence_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2185592</th>\n",
       "      <td>436940</td>\n",
       "      <td>B003ODBTBO</td>\n",
       "      <td>AYUCCFUVEEMAU</td>\n",
       "      <td>donimbo</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1342569600</td>\n",
       "      <td>donimbo</td>\n",
       "      <td>P_436939</td>\n",
       "      <td>S_5</td>\n",
       "      <td>As stated in S. Crocker's review, Keurig is a ...</td>\n",
       "      <td>[-0.58030456, 0.09047128, -0.30690676, -0.2801...</td>\n",
       "      <td>[-0.15631469, -0.29024586, -0.19089317, -0.177...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483613</th>\n",
       "      <td>497385</td>\n",
       "      <td>B001IZIC8I</td>\n",
       "      <td>A2Z2NNP4BCY8F8</td>\n",
       "      <td>Kristina Pearson \"Photogrrl\"</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1261872000</td>\n",
       "      <td>Not What I'd Hoped For</td>\n",
       "      <td>P_497384</td>\n",
       "      <td>S_7</td>\n",
       "      <td>I have no complaints about the quality of the ...</td>\n",
       "      <td>[0.373434, -0.2335425, -0.007979254, -0.015261...</td>\n",
       "      <td>[0.23785596, 0.11807627, 0.13398808, -0.124233...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000519</th>\n",
       "      <td>198645</td>\n",
       "      <td>B002AQL00G</td>\n",
       "      <td>AIXCATW18SQPD</td>\n",
       "      <td>P. Goldberg \"perihope\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350864000</td>\n",
       "      <td>Gluten Free and Good!</td>\n",
       "      <td>P_198644</td>\n",
       "      <td>S_3</td>\n",
       "      <td>I make sure the butter is extremely soft befor...</td>\n",
       "      <td>[0.07756505, 0.047983423, 0.26800308, -0.21137...</td>\n",
       "      <td>[-0.22513467, -0.06171866, -0.3427693, 0.17870...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107435</th>\n",
       "      <td>21798</td>\n",
       "      <td>B000KV61FC</td>\n",
       "      <td>A2G1LRD120SJPC</td>\n",
       "      <td>K. Hill</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1305590400</td>\n",
       "      <td>Surprised at how much my dog liked this!</td>\n",
       "      <td>P_21797</td>\n",
       "      <td>S_6</td>\n",
       "      <td>Yes, I kept both of them, just in case the oth...</td>\n",
       "      <td>[0.12298089, 0.12425774, 0.13720831, -0.082817...</td>\n",
       "      <td>[0.040124238, -0.105401315, 0.028834188, -0.24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483589</th>\n",
       "      <td>294909</td>\n",
       "      <td>B005V9UG18</td>\n",
       "      <td>A3FHULPCEN9DID</td>\n",
       "      <td>T</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1330128000</td>\n",
       "      <td>Wonderful product from a wonderful company</td>\n",
       "      <td>P_294908</td>\n",
       "      <td>S_4</td>\n",
       "      <td>I have bought  these many times.</td>\n",
       "      <td>[0.16828898, 0.06759964, 0.26373923, 0.1195057...</td>\n",
       "      <td>[0.52047026, 0.12971567, 0.16817996, -0.342357...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id   ProductId          UserId                   ProfileName  \\\n",
       "2185592  436940  B003ODBTBO   AYUCCFUVEEMAU                       donimbo   \n",
       "2483613  497385  B001IZIC8I  A2Z2NNP4BCY8F8  Kristina Pearson \"Photogrrl\"   \n",
       "1000519  198645  B002AQL00G   AIXCATW18SQPD        P. Goldberg \"perihope\"   \n",
       "107435    21798  B000KV61FC  A2G1LRD120SJPC                       K. Hill   \n",
       "1483589  294909  B005V9UG18  A3FHULPCEN9DID                             T   \n",
       "\n",
       "         HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "2185592                     2                       2      5  1342569600   \n",
       "2483613                     9                      15      1  1261872000   \n",
       "1000519                     0                       0      5  1350864000   \n",
       "107435                      1                       1      5  1305590400   \n",
       "1483589                     0                       1      5  1330128000   \n",
       "\n",
       "                                            Summary   P_index  \\\n",
       "2185592                                     donimbo  P_436939   \n",
       "2483613                      Not What I'd Hoped For  P_497384   \n",
       "1000519                       Gluten Free and Good!  P_198644   \n",
       "107435     Surprised at how much my dog liked this!   P_21797   \n",
       "1483589  Wonderful product from a wonderful company  P_294908   \n",
       "\n",
       "        S_sentence_number                                           Sentence  \\\n",
       "2185592               S_5  As stated in S. Crocker's review, Keurig is a ...   \n",
       "2483613               S_7  I have no complaints about the quality of the ...   \n",
       "1000519               S_3  I make sure the butter is extremely soft befor...   \n",
       "107435                S_6  Yes, I kept both of them, just in case the oth...   \n",
       "1483589               S_4                   I have bought  these many times.   \n",
       "\n",
       "                                            Summary_vector  \\\n",
       "2185592  [-0.58030456, 0.09047128, -0.30690676, -0.2801...   \n",
       "2483613  [0.373434, -0.2335425, -0.007979254, -0.015261...   \n",
       "1000519  [0.07756505, 0.047983423, 0.26800308, -0.21137...   \n",
       "107435   [0.12298089, 0.12425774, 0.13720831, -0.082817...   \n",
       "1483589  [0.16828898, 0.06759964, 0.26373923, 0.1195057...   \n",
       "\n",
       "                                           Sentence_vector  \n",
       "2185592  [-0.15631469, -0.29024586, -0.19089317, -0.177...  \n",
       "2483613  [0.23785596, 0.11807627, 0.13398808, -0.124233...  \n",
       "1000519  [-0.22513467, -0.06171866, -0.3427693, 0.17870...  \n",
       "107435   [0.040124238, -0.105401315, 0.028834188, -0.24...  \n",
       "1483589  [0.52047026, 0.12971567, 0.16817996, -0.342357...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentence.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4cb71c3-91c8-4609-baff-2b33f942f4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame size: 18868.40 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "size_in_bytes = sys.getsizeof(df_sentence)\n",
    "size_in_mb = size_in_bytes / (1024**2)  # Convert to Megabytes\n",
    "print(\"DataFrame size: {:.2f} MB\".format(size_in_mb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c58ef8ac-a34f-4f30-8b79-fcb1a8464c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c04a4332-d5fc-4626-8c18-7c372656cf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_sentence: 19784953689 bytes\n",
      "df_paragraph: 471976577 bytes\n",
      "__: 34982 bytes\n",
      "_31: 34982 bytes\n",
      "files_df: 2159 bytes\n",
      "tqdm: 2008 bytes\n",
      "BertTokenizer: 2008 bytes\n",
      "BertModel: 2008 bytes\n",
      "_i25: 1252 bytes\n",
      "_i19: 1123 bytes\n",
      "_i17: 1119 bytes\n",
      "_22: 1118 bytes\n",
      "AutoModel: 1064 bytes\n",
      "AutoTokenizer: 1064 bytes\n",
      "_i20: 764 bytes\n",
      "_i8: 484 bytes\n",
      "_i5: 423 bytes\n",
      "_i34: 422 bytes\n",
      "datetime: 408 bytes\n",
      "BytesIO: 408 bytes\n",
      "_ih: 376 bytes\n",
      "In: 376 bytes\n",
      "_oh: 360 bytes\n",
      "Out: 360 bytes\n",
      "csv_files_df: 356 bytes\n",
      "_i14: 339 bytes\n",
      "_i28: 310 bytes\n",
      "_i12: 289 bytes\n",
      "_i9: 280 bytes\n",
      "df_pickle_filename: 255 bytes\n",
      "_ii: 252 bytes\n",
      "_i32: 252 bytes\n",
      "_i11: 215 bytes\n",
      "_i24: 207 bytes\n",
      "filepath: 195 bytes\n",
      "reviews_file_path: 195 bytes\n",
      "_i29: 185 bytes\n",
      "_i30: 172 bytes\n",
      "nan_counts: 162 bytes\n",
      "download_dir: 159 bytes\n",
      "___: 158 bytes\n",
      "_23: 158 bytes\n",
      "_i26: 158 bytes\n",
      "_i6: 156 bytes\n",
      "open: 136 bytes\n",
      "split_paragraphs_into_sentences: 136 bytes\n",
      "split_paragraphs_into_sentences_old: 136 bytes\n",
      "generate_vectors: 136 bytes\n",
      "_i13: 132 bytes\n",
      "download_files: 120 bytes\n",
      "data: 120 bytes\n",
      "__doc__: 113 bytes\n",
      "_i7: 112 bytes\n",
      "_i10: 110 bytes\n",
      "_i18: 108 bytes\n",
      "_i21: 108 bytes\n",
      "__session__: 103 bytes\n",
      "_i4: 80 bytes\n",
      "_i23: 80 bytes\n",
      "_i27: 79 bytes\n",
      "__builtin__: 72 bytes\n",
      "__builtins__: 72 bytes\n",
      "os: 72 bytes\n",
      "pd: 72 bytes\n",
      "np: 72 bytes\n",
      "nltk: 72 bytes\n",
      "torch: 72 bytes\n",
      "sys: 72 bytes\n",
      "gc: 72 bytes\n",
      "_i: 71 bytes\n",
      "_i33: 71 bytes\n",
      "_iii: 70 bytes\n",
      "_i31: 70 bytes\n",
      "_i2: 69 bytes\n",
      "_i22: 68 bytes\n",
      "_i1: 67 bytes\n",
      "_i3: 67 bytes\n",
      "_i15: 67 bytes\n",
      "_i16: 67 bytes\n",
      "_dh: 64 bytes\n",
      "get_ipython: 64 bytes\n",
      "csv_filepaths: 64 bytes\n",
      "file: 60 bytes\n",
      "__name__: 57 bytes\n",
      "filename: 56 bytes\n",
      "_11: 56 bytes\n",
      "_15: 56 bytes\n",
      "_16: 56 bytes\n",
      "file_extension: 53 bytes\n",
      "exit: 48 bytes\n",
      "quit: 48 bytes\n",
      "tokenizer: 48 bytes\n",
      "model: 48 bytes\n",
      "nan_count_paragraph: 32 bytes\n",
      "nan_count_summary: 32 bytes\n",
      "size_in_bytes: 32 bytes\n",
      "_5: 28 bytes\n",
      "_: 24 bytes\n",
      "size_in_mb: 24 bytes\n",
      "_33: 24 bytes\n",
      "__package__: 16 bytes\n",
      "__loader__: 16 bytes\n",
      "__spec__: 16 bytes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Create a list of tuples from the global symbol table to avoid RuntimeError\n",
    "all_objects = [(name, sys.getsizeof(obj)) for name, obj in globals().items()]\n",
    "\n",
    "# Sort the list by size\n",
    "sorted_objects = sorted(all_objects, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the object names and their sizes\n",
    "for name, size in sorted_objects:\n",
    "    print(f\"{name}: {size} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77c609f8-9d60-4562-93f9-9845de0e170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('df_sentence', 19784953689)\n",
      "('df_paragraph', 471976577)\n",
      "('__', 34982)\n",
      "('_31', 34982)\n"
     ]
    }
   ],
   "source": [
    "for element in sorted_objects[0:4]:\n",
    "    print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ab5326f-3b61-4cc6-b06b-4ea797be0395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Id   ProductId          UserId                   ProfileName  \\\n",
      "2185592  436940  B003ODBTBO   AYUCCFUVEEMAU                       donimbo   \n",
      "2483613  497385  B001IZIC8I  A2Z2NNP4BCY8F8  Kristina Pearson \"Photogrrl\"   \n",
      "1000519  198645  B002AQL00G   AIXCATW18SQPD        P. Goldberg \"perihope\"   \n",
      "107435    21798  B000KV61FC  A2G1LRD120SJPC                       K. Hill   \n",
      "1483589  294909  B005V9UG18  A3FHULPCEN9DID                             T   \n",
      "\n",
      "         HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "2185592                     2                       2      5  1342569600   \n",
      "2483613                     9                      15      1  1261872000   \n",
      "1000519                     0                       0      5  1350864000   \n",
      "107435                      1                       1      5  1305590400   \n",
      "1483589                     0                       1      5  1330128000   \n",
      "\n",
      "                                            Summary   P_index  \\\n",
      "2185592                                     donimbo  P_436939   \n",
      "2483613                      Not What I'd Hoped For  P_497384   \n",
      "1000519                       Gluten Free and Good!  P_198644   \n",
      "107435     Surprised at how much my dog liked this!   P_21797   \n",
      "1483589  Wonderful product from a wonderful company  P_294908   \n",
      "\n",
      "        S_sentence_number                                           Sentence  \\\n",
      "2185592               S_5  As stated in S. Crocker's review, Keurig is a ...   \n",
      "2483613               S_7  I have no complaints about the quality of the ...   \n",
      "1000519               S_3  I make sure the butter is extremely soft befor...   \n",
      "107435                S_6  Yes, I kept both of them, just in case the oth...   \n",
      "1483589               S_4                   I have bought  these many times.   \n",
      "\n",
      "                                            Summary_vector  \\\n",
      "2185592  [-0.58030456, 0.09047128, -0.30690676, -0.2801...   \n",
      "2483613  [0.373434, -0.2335425, -0.007979254, -0.015261...   \n",
      "1000519  [0.07756505, 0.047983423, 0.26800308, -0.21137...   \n",
      "107435   [0.12298089, 0.12425774, 0.13720831, -0.082817...   \n",
      "1483589  [0.16828898, 0.06759964, 0.26373923, 0.1195057...   \n",
      "\n",
      "                                           Sentence_vector  \n",
      "2185592  [-0.15631469, -0.29024586, -0.19089317, -0.177...  \n",
      "2483613  [0.23785596, 0.11807627, 0.13398808, -0.124233...  \n",
      "1000519  [-0.22513467, -0.06171866, -0.3427693, 0.17870...  \n",
      "107435   [0.040124238, -0.105401315, 0.028834188, -0.24...  \n",
      "1483589  [0.52047026, 0.12971567, 0.16817996, -0.342357...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(__)  \n",
    "print(type(__))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90a336a8-2695-467f-bd1b-3c57e684145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "44ca1a2f-4a6f-4a40-925c-f54683861b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44c3f315-0ba2-427c-9339-aeebb14b8ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Paragraph'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paragraph.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56a82323-b6de-4ff9-bb21-0c0a7d777d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecaa5132c46484196c04e1197204b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing:   0%|          | 0/568427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_paragraph['Paragraph_vector'] = df_paragraph['Paragraph'].progress_apply(lambda x: generate_vectors(x) if isinstance(x, str) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47771aa8-cbc7-4920-84b3-1c51121b81f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\downloads\\amazon_customer_reviews\\amazon_reviews_pickle_paragraphs_20240219.pkl\n"
     ]
    }
   ],
   "source": [
    "df_pickle_filename = os.path.join(download_dir,\"amazon_reviews_pickle_paragraphs_20240219.pkl\")\n",
    "df_paragraph.to_pickle(df_pickle_filename)\n",
    "print(df_pickle_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa248b42-c106-495e-b9ea-fa9ba47faf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\downloads\\amazon_customer_reviews\\amazon_reviews_pickle_paragraphs_20240219.xlsx\n"
     ]
    }
   ],
   "source": [
    "df_xlsx_filename = os.path.join(download_dir,\"amazon_reviews_pickle_paragraphs_20240219.xlsx\")\n",
    "df_without_vector = df_paragraph.drop('Paragraph_vector', axis=1)\n",
    "#df_paragraph.to_excel(df_xlsx_filename, index=False)\n",
    "df_without_vector.to_excel(df_xlsx_filename, index=False)\n",
    "\n",
    "print(df_xlsx_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ec93689-6c72-45a9-a686-8338c6b1b86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\github\\\\Johnny_Data606'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6289421-2416-4388-ae16-12c81dfd75f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
